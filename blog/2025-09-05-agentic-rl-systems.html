<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Agentic RL Systems: Scaling Training for AI Agents</title>
<script src="https://cdn.tailwindcss.com"></script>
<style>
    :root {
        --primary-color: #37352f;
        --border-color: #e1e5e9;
        --background-color: #ffffff;
        --hover-bg: #f7f6f3;
    }
    body {
        margin-top: 80px;
        background-color: var(--background-color);
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
        line-height: 1.7;
        color: #374151;
    }
    .topnav {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        background-color: rgba(255, 255, 255, 0.98);
        backdrop-filter: blur(12px);
        border-bottom: 1px solid #e5e7eb;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        overflow: hidden;
        z-index: 1000;
        display: flex;
        align-items: center;
        height: 60px;
        padding: 0 24px;
        margin: 0;
    }
    .topnav a {
        display: block;
        color: #374151;
        text-align: center;
        padding: 8px 16px;
        text-decoration: none;
        font-size: 15px;
        font-weight: 500;
        transition: all 0.2s ease;
        border-radius: 8px;
        margin: 0 12px 0 0;
    }
    .topnav a:hover {
        background-color: #f3f4f6;
        color: #1f2937;
    }
    .blog-meta {
        margin: 32px 0;
        padding: 20px 0;
        border-bottom: 1px solid #e5e7eb;
        display: flex;
        justify-content: space-between;
        align-items: center;
        flex-wrap: wrap;
        gap: 16px;
    }
    .blog-meta p {
        margin: 0;
        color: #6b7280;
        font-size: 14px;
    }
    .follow-buttons {
        display: flex;
        gap: 12px;
    }
    .follow-btn {
        display: inline-flex;
        align-items: center;
        gap: 6px;
        padding: 6px 12px;
        border-radius: 6px;
        text-decoration: none;
        font-size: 13px;
        font-weight: 500;
        transition: all 0.2s ease;
        border: 1px solid #e5e7eb;
        background: #ffffff;
        color: #374151;
    }
    .follow-btn:hover {
        background: #f9fafb;
        border-color: #d1d5db;
        transform: translateY(-1px);
    }
    .btn-icon {
        width: 16px;
        height: 16px;
    }
    h1 {
        color: #111827 !important;
        font-size: 2.25rem !important;
        font-weight: 700 !important;
        line-height: 1.2 !important;
        margin: 24px 0 16px 0 !important;
    }
    h2 {
        color: #1f2937 !important;
        font-size: 1.5rem !important;
        font-weight: 600 !important;
        margin: 32px 0 16px 0 !important;
        line-height: 1.3 !important;
    }
    p {
        margin: 16px 0;
        color: #374151;
    }
    ul, ol {
        margin: 16px 0;
        color: #374151;
    }
    li {
        margin: 8px 0;
    }
    .prose {
        max-width: none;
    }
    @media (max-width: 768px) {
        body { margin-top: 70px; }
        .topnav { height: 60px; padding: 0 16px; }
        .topnav a { padding: 8px 12px; font-size: 14px; margin-right: 8px; }
        .blog-meta { flex-direction: column; align-items: flex-start; }
        h1 { font-size: 1.875rem !important; }
    }
</style>
</head>
<body class="bg-white text-black">
<div class="topnav">
<a href="../index.html">Home</a>
<a href="../blog.html">Blog</a>
</div>

<main class="max-w-3xl mx-auto px-4 py-10">
<article class="prose max-w-none">
<h1 class="text-3xl font-bold text-black">Scaling RL Training for AI Agents</h1>
<!-- <p class="text-sm text-gray-600">Posted on September 5, 2025 by Jiachen Liu</p> -->

<div class="blog-meta">
    <p>Posted on September 5, 2025 by Jiachen Liu</p>  
     <div class="follow-buttons">
        <a href="https://x.com/JIACHENLIU8" target="_blank" class="follow-btn">
            <svg class="btn-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"/>
            </svg>
            X
        </a>
        <a href="https://www.linkedin.com/in/jiachen-amber-liu-872506169/" target="_blank" class="follow-btn">
            <svg class="btn-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z" fill="currentColor"/>
            </svg>
            LinkedIn
        </a>
    </div>
</div>

<section class="mt-8 p-4 border-l-4 border-blue-300 bg-blue-50">
<h2 class="text-xl font-semibold m-0">TL;DR</h2>
<p class="mt-2">The paradigm for training LLMs has shifted from single-response tasks to complex, multi-step problem-solving driven by AI agents. Traditional RL systems for LLMs are not built for this, especially under heavy, heterogeneous environment demands. This post answers three questions:</p>
<ul class="list-disc pl-6">
<li>How is RL for LLM-based agents different from RL for text generation?</li>
<li>What are the critical system challenges for agentic RL?</li>
<li>What solutions are emerging from research and industry?</li>
</ul>
</section>

<section class="mt-10">
<p>This year, with the rise of AI agents, the frontier of AI has moved from single-response generation toward solving complex, multi-step problems. Researchers are building “Agentic Intelligence” — the ability to autonomously plan, reason, and act within dynamic environments. This requires models that can strategize for long horizons, use tools (code interpreters, web search), and adapt to environmental feedback.</p>
<p>Think of LLMs as the “brain” and LLM-based agents as the “body and hands.” Earlier work focused on the brain (reasoning). To solve real tasks, the brain must direct actions through a body: interacting with sandboxes, executing code, browsing the web, or running experiments. For instance, a scientific discovery agent may autonomously design and execute ML experiments on GPUs, while a coding agent must compile and run code in isolated containers. This demands RL training pipelines purpose-built for long-horizon, tool-rich, open-ended environments.</p>
</section>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">The Bottleneck: Why Existing RL Frameworks Fall Short</h2>
<p>Simply wiring agent rollouts into traditional LLM RL frameworks doesn’t work. These systems were designed for simple, stateless rollouts and crumble under the diverse and demanding needs of agents.</p>
<p>Agents require both brain and body. While the LLM handles reasoning, the agent’s hands involve external environments, APIs, or compute resources, each with heavy and heterogeneous requirements:</p>
<ul class="list-disc pl-6">
<li>A coding agent needs an isolated Docker container with specific dependencies.</li>
<li>An ML engineering agent may require dedicated GPUs and long-running experiments.</li>
<li>A web search or browser agent needs stateful, sandboxed sessions.</li>
</ul>
<p>Running modest batches (e.g., 128 parallel rollouts) on a single node is infeasible if each requires a dedicated container or specialized resource. Conversely, very small batches (e.g., 8) underutilize LLM serving and slow rollouts.</p>
</section>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">The Decoupled Solution: Introducing the Agent Layer</h2>
<p>A practical design introduces a dedicated Agent Layer between the RL framework (inference and training engines) and agent execution environments. It acts as a scheduler and orchestrator for agent tasks.</p>
<ul class="list-disc pl-6">
<li><span class="font-semibold">RL Framework</span>: trains the model and serves LLM inference via a standard API.</li>
<li><span class="font-semibold">Agent Execution Environments</span>: run independently on distributed machines, providing sandboxes and tools.</li>
<li><span class="font-semibold">Agent Layer</span>: dispatches rollouts to environments, provides inference endpoints, and collects trajectory data into a replay buffer.</li>
</ul>
<figure class="my-8">
  <img src="../imag/rl-sys-overview.png" alt="Agentic RL System Overview" style="width: 80%; max-width: 700px; margin: 0 auto; display: block; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.08);">
  <figcaption style="text-align: center; color: #555; font-size: 1em; margin-top: 1em;">
    Figure: Overview of the Agent Layer in Agentic RL Systems
  </figcaption>
</figure>

<p>This decoupled architecture underpins agentic RL at scale. Below are three major challenges and emerging solutions.</p>
</section>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">Challenge 1: Integrating Diverse Agents and Frameworks</h2>
<p>Agent performance is tightly coupled to its implementation — prompting scaffold, tool integrations, and environments. A model trained with one agent implementation may not generalize to another. The training system must support diverse agents without major agent-side changes.</p>
<p>A critical function of the Agent Layer is automatic trajectory capture via a <span class="font-semibold">Unified Data Interface</span>. By instrumenting the agent runtime (e.g., tracing LLM API calls), the system can capture stepwise trajectories with:</p>
<ul class="list-disc pl-6">
<li><span class="font-semibold">State</span>: snapshot of key environment variables.</li>
<li><span class="font-semibold">Action</span>: LLM outputs (tool calls, answers).</li>
<li><span class="font-semibold">Reward</span>: quality of actions or final outcomes.</li>
</ul>
<p>This standardized format decouples agent logic from the RL framework. As noted in Agent Lightning, the trainer becomes “agent-agnostic” and the agent “trainer-agnostic.” GLM-4.5 exposes a unified HTTP endpoint for writing trajectories to a shared data pool, enabling task-specific filtering and adaptive sampling. Kimi K2 and Kimi-Researcher adopt OpenAI Gym-like interfaces to add new tasks and environments efficiently.</p>
</section>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">Challenge 2: Environment Management and Rollout Scalability</h2>
<p>Training and evaluating agentic LLMs requires massive parallel rollouts (e.g., batch size 128, multiple generations per prompt) across simulated or real environments. These dynamic environments demand orchestration for scheduling, checkpointing, fault tolerance, and reproducibility.</p>
<ul class="list-disc pl-6">
<li><span class="font-semibold">Remote Execution Services</span>: Master/worker schedulers dispatch tasks to large pools of remote workers (e.g., rStar2-Agent, SkyRL-v0), decoupling environment interaction from training and enabling parallelism.</li>
<li><span class="font-semibold">Efficient Sandbox Infrastructure</span>: Docker/Kubernetes provision isolated environments per run (e.g., Kimi-Researcher, GLM-4.5). Systems like Daytona simplify container management. SkyRL reports 80–100 containers per replica on 16-CPU nodes via lightweight runtimes and image caching.</li>
<li><span class="font-semibold">Centralized Environment Pools</span>: For stateful tools (filesystems, browsers), a pool allocates an environment per task and returns it upon completion (e.g., AgentFly) to minimize setup latency.</li>
</ul>
</section>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">Challenge 3: Handling Long and Complex Tasks</h2>
<p>Agentic tasks are heterogeneous and unpredictable; some finish quickly, others require dozens of steps. Long-tail tasks can block training and leave GPUs idle.</p>
<ul class="list-disc pl-6">
<li><span class="font-semibold">Asynchronous & Decoupled Architecture</span>: Partition resources into rollout engines (producers) and training engines (consumers) with a shared data pool or replay buffer (e.g., GLM-4.5, Kimi-Researcher, rLLM). SkyRL further pipelines initialization, rollout, and reward calculation.</li>
<li><span class="font-semibold">Partial Rollouts</span>: Pause long tasks, save state, and resume with updated weights (used by Kimi K2 and Kimi-Researcher) for significant speedups.</li>
<li><span class="font-semibold">Dynamic Load Balancing</span>: Assign rollouts to GPUs based on real-time KV cache availability (e.g., rStar2-Agent) to prevent idle time and cache overflows.</li>
</ul>
</section>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">The Road Ahead</h2>
<p>We are moving toward agents that help complete real-world tasks, not just operate in sandboxes. The systems discussed here are foundational, but more is needed:</p>
<ul class="list-disc pl-6">
<li><span class="font-semibold">Algorithmic Advances</span>: Address sparse rewards, credit assignment, and sample efficiency.</li>
<li><span class="font-semibold">Agent-Aware Scheduling</span>: Optimize allocation based on task-specific resource and runtime profiles.</li>
<li><span class="font-semibold">Multi-Agent Systems</span>: Enable collaboration and competition among agents.</li>
<li><span class="font-semibold">Decentralized Agentic RL</span>: Learn continuously from end-user feedback while ensuring privacy and safe exploration.</li>
<li><span class="font-semibold">Embodied Agents & Robotics</span>: Extend from sandboxes to the physical world with strict simulation and control requirements.</li>
</ul>
<p class="mt-6">The shift from “LLMs that think” to “agents that act” demands new abstractions. Decoupling model training/inference from execution via an Agent Layer, unified trajectory formats, remote execution pools, and asynchronous pipelines enables scaling agentic RL without letting environment complexity overwhelm training.</p>
</section>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">References</h2>
<ol class="list-decimal pl-6">
<li>Kimi K2: Open Agentic Intelligence</li>
<li>Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities</li>
<li>GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</li>
<li>AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents</li>
<li>rStar2-Agent: Agentic Reasoning Technical Report</li>
<li>Daytona: Sandbox Infrastructure for Reinforcement Learning Agents</li>
<li>SkyRL-v0: Train Real-World Long-Horizon Agents via Reinforcement Learning</li>
<li>Agent Lightning: Train ANY AI Agents with Reinforcement Learning</li>
<li>rLLM: A Framework for Post-Training Language Agents</li>
</ol>
</section>

</article>
</main>

<footer class="border-t border-blue-200 py-6">
<div class="max-w-4xl mx-auto px-4 text-sm text-gray-600">© AmberLJC</div>
</footer>

</body>
</html>
