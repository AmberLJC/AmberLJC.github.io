<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>When LLMs Grow Hands and Feet, How to Design our Agentic RL Systems?</title>
<script src="https://cdn.tailwindcss.com"></script>
<style>
    :root {
        --primary-color: #37352f;
        --border-color: #e1e5e9;
        --background-color: #ffffff;
        --hover-bg: #f7f6f3;
    }
    body {
        margin-top: 80px;
        background-color: var(--background-color);
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
        line-height: 1.7;
        color: #374151;
    }
    .topnav {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        background-color: rgba(255, 255, 255, 0.98);
        backdrop-filter: blur(12px);
        border-bottom: 1px solid #e5e7eb;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        overflow: hidden;
        z-index: 1000;
        display: flex;
        align-items: center;
        height: 60px;
        padding: 0 24px;
        margin: 0;
    }
    .topnav a {
        display: block;
        color: #374151;
        text-align: center;
        padding: 8px 16px;
        text-decoration: none;
        font-size: 15px;
        font-weight: 500;
        transition: all 0.2s ease;
        border-radius: 8px;
        margin: 0 12px 0 0;
    }
    .topnav a:hover {
        background-color: #f3f4f6;
        color: #1f2937;
    }
    .blog-meta {
        margin: 32px 0;
        padding: 20px 0;
        border-bottom: 1px solid #e5e7eb;
        display: flex;
        justify-content: space-between;
        align-items: center;
        flex-wrap: wrap;
        gap: 16px;
    }
    .blog-meta p {
        margin: 0;
        color: #6b7280;
        font-size: 14px;
    }
    .follow-buttons {
        display: flex;
        gap: 12px;
    }
    .follow-btn {
        display: inline-flex;
        align-items: center;
        gap: 6px;
        padding: 6px 12px;
        border-radius: 6px;
        text-decoration: none;
        font-size: 13px;
        font-weight: 500;
        transition: all 0.2s ease;
        border: 1px solid #e5e7eb;
        background: #ffffff;
        color: #374151;
    }
    .follow-btn:hover {
        background: #f9fafb;
        border-color: #d1d5db;
        transform: translateY(-1px);
    }
    .btn-icon {
        width: 16px;
        height: 16px;
    }
    h1 {
        color: #111827 !important;
        font-size: 2.25rem !important;
        font-weight: 700 !important;
        line-height: 1.2 !important;
        margin: 24px 0 16px 0 !important;
    }
    h2 {
        color: #1f2937 !important;
        font-size: 1.5rem !important;
        font-weight: 600 !important;
        margin: 32px 0 16px 0 !important;
        line-height: 1.3 !important;
    }
    p {
        margin: 16px 0;
        color: #374151;
    }
    ul, ol {
        margin: 16px 0;
        color: #374151;
    }
    li {
        margin: 8px 0;
    }
    .prose {
        max-width: none;
    }
    .challenge-block {
        background: linear-gradient(135deg, #fafbfc 0%, #f3f4f6 100%);
        border-left: 4px solid #6365f1b4;
        border-radius: 0 12px 12px 0;
        padding: 28px;
        margin: 28px 0;
        box-shadow: 0 2px 12px rgba(99, 102, 241, 0.08);
        border: 1px solid rgba(99, 102, 241, 0.12);
        border-left: 4px solid #4e8eec90;
    }
    .challenge-block h2 {
        margin-top: 0 !important;
    }
    @media (max-width: 768px) {
        body { margin-top: 70px; }
        .topnav { height: 60px; padding: 0 16px; }
        .topnav a { padding: 8px 12px; font-size: 14px; margin-right: 8px; }
        .blog-meta { flex-direction: column; align-items: flex-start; }
        h1 { font-size: 1.875rem !important; }
        .challenge-block { padding: 16px; }
    }
</style>
</head>
<body class="bg-white text-black">
<div class="topnav">
<a href="../index.html">Home</a>
<a href="../blog.html">Blog</a>
</div>

<main class="max-w-3xl mx-auto px-4 py-10">
<article class="prose max-w-none">
<h1 class="text-3xl font-bold text-black">When LLMs Grow Hands and Feet, How to Design our Agentic RL Systems?</h1>


<div class="blog-meta">
    <p>Posted on September 5, 2025 by Jiachen Liu</p>  
     <div class="follow-buttons">
        <a href="https://x.com/JIACHENLIU8" target="_blank" class="follow-btn">
            <svg class="btn-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"/>
            </svg>
            X
        </a>
        <a href="https://www.linkedin.com/in/jiachen-amber-liu-872506169/" target="_blank" class="follow-btn">
            <svg class="btn-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z" fill="currentColor"/>
            </svg>
            LinkedIn
        </a>
    </div>
</div>

<section class="mt-8 p-4 border-l-4 border-blue-300 bg-blue-50">
<h2 class="text-xl font-semibold m-0">TL;DR</h2>
<p class="mt-2">The paradigm for training LLMs has shifted from single-response tasks to complex, multi-step problem-solving driven by AI agents. Previous Reinforcement Learning (RL) systems for chat LLM are ill-equipped for this new paradigm because they can't handle the heavy computational and resource needs of agentic tasks. This blog post answers three key questions:</p>
<ul class="list-disc pl-6">
<li>How is RL for LLM-based agents different from traditional RL for chat LLM?</li>
<li>What are the critical system challenges in adapting RL systems for LLM-based agents?</li>
<li>What solutions are top research labs or industry developing to address these challenges?</li>
</ul>
</section>

<section class="mt-10">
<p>This year, with the rise of AI agents, the frontier of AI has moved from single-response generation toward solving complex, multi-step problems. Researchers start developing "Agentic Intelligence"—the ability to autonomously plan, reason, and act within dynamic environments. This evolution requires models that can strategize for long-horizon tasks, use tools like code interpreters and web search, and adapt based on environmental feedback.</p>
<p>A useful analogy is to think of LLMs as the "brain" and the LLM-based agent as the "body and hands." In the early phase of LLM development, research focused almost exclusively on the brain—refining reasoning ability. But to solve real tasks, the brain must now direct actions through a body: interacting with sandboxes, executing code, browsing the web, or running experiments. For instance, a scientific discovery agent may need to autonomously design and execute machine learning experiments on GPUs, while a coding agent must safely compile and run code inside isolated containers. This new level of capability requires RL training pipelines purpose-built for long-horizon, tool-rich, open-ended environments.</p>
</section>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">The Bottleneck: Why Existing RL Frameworks Fall Short</h2>
<p>Simply plugging the AI agent rollout into a traditional LLM RL framework doesn't work. These frameworks were designed for simple, stateless LLM rollouts and crumble under the diverse and demanding needs of agents.</p>
<p>The challenge is that agents require both brain and body: while the LLM handles reasoning, the agent's "hands" involve external environments, APIs, or compute resources. Each environment may impose heavy and heterogeneous requirements:</p>
<ul class="list-disc pl-6">
<li>A coding agent needs an isolated Docker container with a specific file system and dependencies to safely execute code.</li>
<li>An ML engineering agent might require dedicated GPU access and run long-running experiments.</li>
<li>A web search agent …</li>
</ul>
<p>Running even modest batches of such agents (e.g., 128 parallel rollouts) on a local node is impossible if each requires a dedicated Docker container or specialized resource. On the other hand, because of local constraints, existing frameworks run very small batches (e.g., 8), which underutilizes the LLM serving systems and slows down the agent rollout.</p>
</section>

<table class="w-full border-collapse border border-gray-300 mt-6 mb-6">
<thead>
<tr class="bg-gray-50">
<th class="border border-gray-300 px-4 py-2 text-left font-semibold">Feature</th>
<th class="border border-gray-300 px-4 py-2 text-left font-semibold">Traditional LLM RL (The "Brain")</th>
<th class="border border-gray-300 px-4 py-2 text-left font-semibold">Agentic RL (The "Brain and Body")</th>
</tr>
</thead>
<tbody>
<tr>
<td class="border border-gray-300 px-4 py-2 font-semibold">Primary Goal</td>
<td class="border border-gray-300 px-4 py-2">Optimize single‑turn language quality (helpfulness, style, safety) via preference/reward fine‑tuning.</td>
<td class="border border-gray-300 px-4 py-2">Solve complex, multi-step problems autonomously in a dynamic environment.</td>
</tr>
<tr class="bg-gray-50">
<td class="border border-gray-300 px-4 py-2 font-semibold">Task Horizon</td>
<td class="border border-gray-300 px-4 py-2"><strong>Single turn & stateless.</strong> A single prompt leads to a single response.</td>
<td class="border border-gray-300 px-4 py-2"><strong>Multi-turn & stateful.</strong> An agent takes a sequence of actions, and its state persists across steps.</td>
</tr>
<tr>
<td class="border border-gray-300 px-4 py-2 font-semibold">Interaction Model</td>
<td class="border border-gray-300 px-4 py-2">The LLM generates text. A reward model scores the final output.</td>
<td class="border border-gray-300 px-4 py-2">The agent uses tools, calls APIs, executes code, and interacts with external systems.</td>
</tr>
<tr class="bg-gray-50">
<td class="border border-gray-300 px-4 py-2 font-semibold">Resource Demand</td>
<td class="border border-gray-300 px-4 py-2">Lightweight (prompt + reward model).</td>
<td class="border border-gray-300 px-4 py-2">Heavyweight, diverse, and external (code interpreters, sandbox, web browsers).</td>
</tr>
<tr>
<td class="border border-gray-300 px-4 py-2 font-semibold">Key System Bottleneck</td>
<td class="border border-gray-300 px-4 py-2">LLM inference throughput and reward model scoring.</td>
<td class="border border-gray-300 px-4 py-2">Orchestrating and scaling diverse, resource-intensive environments for parallel rollouts.</td>
</tr>
</tbody>
<caption class="text-sm text-gray-600 mb-2 font-medium">Table 1: A comparison of system demands between LLM RL and Agentic RL.</caption>
</table>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">The Decoupled Solution: Introducing the "Agent Layer"</h2>
<p>To solve these challenges, a new system design is emerging that introduces a dedicated Agent Layer. This layer sits between the RL framework (including the inference engine and training engine) and the agent's execution environment, acting as a specialized scheduler and orchestrator for agent tasks.</p>
<ul class="list-disc pl-6">
<li><span class="font-semibold">The RL Framework</span> focuses on what it does best: training the model and serving LLM inference requests via a standard API.</li>
<li><span class="font-semibold">The Agent Execution Environments</span> run independently on distributed machines, providing the sandboxes and tools the agent needs.</li>
<li><span class="font-semibold">The Agent Layer</span> is the bridge. It dispatches rollout tasks to agent environments, provides them with the API endpoint for LLM inference, and collects the resulting trajectory data to send back to a replay buffer for the trainer.</li>
</ul>
<figure class="my-8">
  <img src="../imag/rl-sys-overview.png" alt="Agentic RL System Overview" style="width: 80%; max-width: 700px; margin: 0 auto; display: block; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.08);">
  <figcaption style="text-align: center; color: #555; font-size: 1em; margin-top: 1em;">
    Figure 1: Conceptual Diagram of the Agent Layer in Agentic RL Systems
  </figcaption>
</figure>

<p>This decoupled architecture underpins agentic RL at scale. Below are three major challenges and emerging solutions.</p>
</section>

<div class="challenge-block">
<h2 class="text-2xl font-semibold text-black">Challenge 1: Integrating Diverse Agents and RL Frameworks 🧩</h2>
<p>The performance of an agentic LLM is deeply tied to its underlying implementation—its prompting scaffold, tool integrations, and environments. A LLM trained with one agent implementation may struggle to generalize to another with a different prompt structure or tool definition. To develop generalized agentic LLMs, the RL training system must support diverse agent implementation without requiring significant code change on the agent side.</p>
<p>Therefore, a critical function of the Agent Layer is to automatically capture agent trajectories for any agent implementation. This is often achieved through a <span class="font-semibold">Unified Data Interface</span>. By instrumenting the agent runtime (e.g., by tracing LLM API calls), the system can capture every agent's step. These structured trajectories contain the sequence of states, actions, and rewards from the agent's run.</p>
<ul class="list-disc pl-6">
<li><span class="font-semibold">State</span>: A snapshot of all critical variables in the agent's environment at a given time.</li>
<li><span class="font-semibold">Action</span>: The output generated by the LLM, such as a tool call or a final answer.</li>
<li><span class="font-semibold">Reward</span>: A signal indicating the quality of an action or the final outcome.</li>
</ul>
<p>This standardized format decouples the agent's implementation logic from the RL framework. The RL framework doesn't need to know how an agent built with LangGraph works; it just consumes the standardized trajectory data. As noted in the Agent-Lightning paper, this design makes the trainer "agent-agnostic" and the agent "trainer-agnostic" [8]. Similarly, GLM-4.5 provides a unified HTTP endpoint, allowing different agent frameworks to write trajectories to a shared data pool [3]. The data pool enables tailored, task-specific filtering and adaptive sampling methods to provide high-quality RL training data for a wide range of tasks. Finally, both Kimi K2 and Kimi-Researcher use a unified, OpenAI Gym-like interface to streamline the addition of new environments and tasks [1, 2].</p>
</div>
<figure class="my-8">
  <img src="../imag/agent-robot.png" alt="Agentic RL with Embodied Agents" style="width: 70%; max-width: 600px; margin: 0 auto; display: block; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.08);">
  <figcaption style="text-align: center; color: #555; font-size: 1em; margin-top: 1em;">
    Figure 2: Generalization of Agentic LLMs.
  </figcaption>
</figure>

<div class="challenge-block">
<h2 class="text-2xl font-semibold text-black">Challenge 2: Environment Management and Agent Rollout Scalability 🖥️</h2>
<p>Training and evaluating agentic LLMs requires massive parallel agent rollouts (e.g. rollout batch size 128 with number of generation per prompt 4) across simulated or real environments. Unlike RL for LLM, agentic RL often involves complex, dynamic environments such as sandboxed simulators, external APIs, or sandboxed real-world interfaces, all of which demand careful orchestration of resources and reproducibility. Managing thousands of concurrent environments introduces difficulties in distributed scheduling, state checkpointing, fault tolerance, and reproducibility.</p>
<p>The solution is to offload agent task execution to a dedicated, isolated service that runs separately from the RL training loop.</p>
<ul class="list-disc pl-6">
<li><span class="font-semibold">Remote Execution Services</span>: Systems like rStar2-Agent and SkyRL use a master/worker architecture where a central scheduler dispatches tasks to a large pool of remote execution workers [5, 7]. This prevents environment interactions from blocking the main training loop and enables massive parallelism.</li>
<li><span class="font-semibold">Efficient Sandbox Infrastructure</span>: Technologies like Docker and Kubernetes are used to provision isolated environments for each agent run. This practice is highlighted by Kimi-Researcher and GLM-4.5 [2, 3]. Frameworks like Daytona further abstract away the complexities of container management, providing simple APIs for environment provisioning [6]. SkyRL [7] designs a Kubernetes-based setup with storage-optimized instances to cache container images, aidocker + crun runtime for lightweight container execution, which is able to run 80–100 containers per replica on 16-CPU nodes.</li>
<li><span class="font-semibold">Centralized Environment Pools</span>: For stateful tools like a file system or browser, each task needs its own dedicated environment. AgentFly describes a centralized system that maintains pools of available environments. When a task starts, an environment is allocated from the pool and returned once the task is complete [4]. An environment is allocated to a task and returned to the pool upon completion, minimizing setup latency.</li>
</ul>
</div>

<div class="challenge-block">
<h2 class="text-2xl font-semibold text-black">Challenge 3: Handling Long and Complex Tasks ⏳</h2>
<p>Agentic tasks are heterogeneous and unpredictable; some finish quickly, while others require dozens of steps and extensive interaction. This variability creates a "long-tail" problem, where a few very long tasks can block the entire training process, leaving expensive GPUs idle while waiting for the slowest rollouts to finish.</p>

<ul class="list-disc pl-6">
<li><span class="font-semibold">Asynchronous & Decoupled Architecture</span>: A popular design, used by GLM-4.5, Kimi-Researcher, and rLLM, is to partition resources into dedicated rollout engines and training engines [2, 3, 9]. The rollout engines act as producers, continuously generating trajectories and feeding them into a central data pool or replay buffer. The training engines are consumers, asynchronously pulling batches of data from this pool to update the model. SkyRL decomposes agent rollout into a fine-grained three-stage producer-consumer pipeline (initialize, rollout, reward calculation) to maximize parallelism [7].</li>
<li><span class="font-semibold">Partial Rollouts</span>: For exceptionally long tasks, the "partial rollout" technique is effective. Instead of waiting for a task to finish, the system can pause it, save its state, and resume it in a future iteration with updated model weights. This simple but powerful trick, used by Kimi K2 and Kimi-Researcher, can yield significant speedups [1, 2].</li>
<li><span class="font-semibold">Dynamic Load Balancing</span>: Statically distributing rollouts evenly across GPUs is inefficient. A more advanced approach, detailed by rStar2-Agent, is a dynamic, load-balanced scheduler [5]. This scheduler assigns rollout requests to GPUs based on their real-time available KV cache capacity. This ensures a balanced workload, preventing both GPU idle time and cache overflows that lead to wasted computation.</li>
</ul>
</div>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">The Road Ahead</h2>
<p>We are moving towards a future where AI agents don't just think or operate in sandboxes; they help us complete real-world tasks. The solutions of agentic RL systems discussed here are foundational pieces, but not sufficient. Looking forward, agents will have the access to real compute resources to conduct experiments and solve problems autonomously. Several trends are pointing in this direction:</p>
<ul class="list-disc pl-6">
<li><span class="font-semibold">Algorithmic Advances</span>: System improvements alone cannot solve the challenges of sparse rewards, credit assignment, and sample efficiency.</li>
<li><span class="font-semibold">Agent-Aware Scheduling</span>: Creating schedulers that understand the specific resource needs and runtime characteristics of different agentic tasks to optimize resource allocation.</li>
<li><span class="font-semibold">Multi-Agent Systems</span>: Developing systems where multiple agents collaborate or compete to solve even more complex problems.</li>
<li><span class="font-semibold">Decentralized Agentic RL</span>: Imagine distributing agent rollouts directly to end-users. This would allow agents to learn continuously from human feedback in real-world applications, creating a powerful, personalized learning loop. This, however, brings significant challenges in privacy, security, and ensuring safe exploration.</li>
<li><span class="font-semibold">Embodied agents & robotics</span>: Extending agentic RL from sandboxes to the physical world introduces hard requirements: complex simulation/real environment, sample efficiency, low-latency control loops with the agent, etc.</li>
</ul>
<p class="mt-6">The shift from "LLMs that think" to "agents that act" demands new system abstractions. A resilient design pattern is to decouple model training/inference from execution using an Agent Layer, unified trajectory formats, remote execution pools, and asynchronous pipelines. These pieces together let researchers and engineers scale agentic RL without letting environment complexity overwhelm model training.</p>
</section>

<section class="mt-10">
<h2 class="text-2xl font-semibold text-black">References</h2>
<ol class="list-decimal pl-6">
<li><a href="https://arxiv.org/abs/2507.20534" target="_blank">Kimi K2: Open Agentic Intelligence</a></li>
<li><a href="https://moonshotai.github.io/Kimi-Researcher/" target="_blank">Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities</a></li>
<li>GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</li>
<li><a href="https://arxiv.org/pdf/2507.14897" target="_blank">AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents</a></li>
<li>rStar2-Agent: Agentic Reasoning Technical Report</li>
<li>Daytona: Sandbox Infrastructure for Reinforcement Learning Agents</li>
<li>SkyRL-v0: Train Real-World Long-Horizon Agents via Reinforcement Learning</li>
<li>Agent Lightning: Train ANY AI Agents with Reinforcement Learning</li>
<li>rLLM: A Framework for Post-Training Language Agents</li>
</ol>
</section>

</article>
</main>

<footer class="border-t border-blue-200 py-6">
<div class="max-w-4xl mx-auto px-4 text-sm text-gray-600">© AmberLJC</div>
</footer>

</body>
</html>
