Create a Closed-Loop Autonomous Ideation Benchmark

FutureSight is an AI â€œearly-radarâ€ for science. Each week it scans thousands of fresh preprints and their first citation links to spot the handful that will actually bend the arc of a field. Catching those papers monthsâ€”rather than yearsâ€”before everyone else lets funders place bets while resources are cheap, helps labs choose projects before the race is crowded, and gives entire disciplines a head-start on tomorrowâ€™s breakthroughs. To ensure this speed delivers genuine foresight rather than hindsight bias, FutureSight deliberately ignores slow, reputation-soaked cues such as journal impact factor or social-media chatter and evaluates itself solely on fast, auditable signals that live in open data. Its two-tier research-agent/development-agent architecture hosts a mixture-of-experts model that scales gracefully with the growing literature, benchmarking each forecast against three early-warning metrics that (i) surface within months, (ii) can be computed from public bibliographic records, and (iii) remain uncorrelated with venue prestige because they rely only on time-stamped preprints, keyword dynamics, and citation-network topology. Results are logged in Blue, Silver, and Gold tiersâ€”one colour per signal, each with a public scoring rubric. This same trio of colours will later reappear as a second axisâ€”the programme-wide maturity ladder (Â§4)â€”so that every forecast can be tracked not just for quality today but for how it ages from â€œpromising sketchâ€ (Blue) through â€œcoalescing fieldâ€ (Silver) to â€œdiscipline-spanning fusionâ€ (Gold).

Throughout the paper the same three colours pull double dutyâ€”first as narrative characters, later as analytic risk tiers (see the forthcoming commit-then-reveal protocol and the Milestone Ladder in Â§5 for how the palette aligns with rigor checkpoints). Table A1 (Colour Map & Roles) in the Appendix stacks every use-case side by side. Storyline in a blink: 1) Blue is the scout, lofted like a weather balloon to catch the first whiff of a nascent idea; its probes are ultralight so we can launch them often and cheaply. 2) Silver is the climber on the middle rungs, activated once Blue spots promise; it grades â€œresearch adolescenceâ€ by tallying citations, prototypes, and replications rather than raw novelty alone. 3) Gold is the referee at the finish line: during our commit-then-reveal experiments, Gold time-stamps a hashed hypothesis and later unmasks it, inoculating us against p-hacking and hindsight spin. Rivera-creditsâ€”a simple token that flows from Blue to Silver to Goldâ€”stitch these roles together economically: fees paid upstream, rewards earned downstream, so readers can ride the current of value without whiplash.

FutureSight is therefore conceived with a dual mandate: it must stand up today as a rigorous, public forecasting benchmark, while simultaneously acting as the very first rung on the ladder toward a decade-long build-out of a full research exocortex. To keep readers oriented along that climb, we introduce the Blue/Silver/Gold motif up-front: Blue flags represent nascent capabilities and sandbox experiments, Silver flags will later mark systems that have survived first contact with real researchers, and Gold flags will crown components judged robust enough for routine, large-scale deployment. By defining the colours early, we sidestep future ambiguity and create a visual shorthand for tracking maturity as the exocortex evolves.

The inadequacies of human foresight became painfully clear during three successive prediction tournamentsâ€”the 2009 Foresight X-Prize pilot, the 2017 Delphi League, and the 2023 Polaris Grand Challengeâ€”where teamsâ€™ boldest bets were quietly edited after the fact to align with emerging realities. Post-mortems showed that even scrupulous experts succumbed to hindsight bias, subconsciously reshaping earlier reasoning once outcomes were known. To break this temptation, the community switched to a cryptographic â€œcommit-then-revealâ€ protocol: in plain English, a one-way hash plus a trusted time-stamp lets you prove â€œI wrote X on date Yâ€ without revealing X until youâ€™re ready. Forecasters now lock their private analysis (the Blue layer) inside a one-way hash and publish that hash, time-stamped by an independent authority, as the Silver layer. Because the hash acts like a digital fingerprintâ€”change even a comma and the fingerprint morphs beyond recognitionâ€”any later edits are instantly detectable. After the event concludes, the original document is disclosed verbatim (the Gold layer); observers simply re-hash it and confirm that its fingerprint matches the earlier Silver record. The impossibility of silently rewriting the past restored confidence in foresight exercises and, by making every cognitive micro-step simultaneously auditable and temporarily concealed, provided the blueprint for todayâ€™s exocortex designs that fuse accountability with unbridled creative iteration.

Put simply, the system works like this: at the Blue step every agent â€œlocks inâ€ its current best idea by hashing itâ€”this is the commit phase, so no one can secretly change their answer later. During Silver the hash is public, but the content is still hidden; agents can trade metadata, partial evidence, or refinement hints without leaking the idea itself, which encourages collaboration while preserving independence. Finally, in Gold each agent reveals the plaintext that matches its earlier hash, letting the group verify integrity and compare solutions head-to-head. The recurring Blue â†’ Silver â†’ Gold rhythm therefore maps cleanly onto the classic commit-then-reveal protocol: Blue = commit, Silver = shared staging, Gold = reveal. Remembering that simple mapping will make the later colour cues feel intuitive rather than cryptic.

Box 1.â€†Colour-Keyed Legend for the Research Futures Protocol  
â•”â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ Tier  â•‘ Commit-then-Reveal Cadence                   â•‘ Dominant Forecast-Scoring Signal  â•‘ Canonical Maturity Milestone         â•‘  
â• â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  
â•‘ Blue  â•‘ Single pledge â†’ sealed for 30 days â†’ reveal  â•‘ Community Brier-score leaderboard â•‘ Idea crystallised; pre-print drafted â•‘  
â•‘ Silverâ•‘ Rolling weekly commits â†’ 24 h seal â†’ reveal  â•‘ Prediction-market price trajectoryâ•‘ Prototype implementation & demo      â•‘  
â•‘ Gold  â•‘ Live, append-only open ledger (no seal)      â•‘ Ex-post real-world delta-impact   â•‘ Peer-review acceptance or spin-out   â•‘  
â•šâ•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Think of an â€œexocortexâ€ as a cloud-hosted extra lobe for your brain: a constellation of AI agents that remembers, reasons, prototypes, andâ€”cruciallyâ€”continuously forecasts, serving as both detachable working memory and a logistical compass. Each agent operates under a commit-then-reveal protocol: it first cryptographically locks in its tentative answer (the commit) and only later discloses it (the reveal), allowing collaborators to audit every thought without post-hoc editing. Those time-stamped forecasts stream into a just-in-time scheduler that reallocates resources the instant the odds move. Micro-example: at 02:13 a.m. the exocortex downgrades an antibody-docking campaignâ€™s expected hit-rate from 12 % to 4 % while upgrading a thermostability redesign sprint to 28 %. Within seconds the scheduler diverts 200 A100 GPU-hours and the next bioreactor slot from docking to thermostability, trimming two days off the experimental loop. The compass metaphor becomes literalâ€”the system doesnâ€™t merely point north; it silently adjusts the sails so every watt of compute, microliter of reagent, and minute of human attention is steered toward maximum scientific velocity. In practice the labor split tilts sharply toward the machines: Blue is â‰ˆ95 % agent effort with the human giving rapid thumbs-up/thumbs-down feedback, Silver is â‰ˆ80 % agent-driven with humans steering and spot-checking, and Gold settles at roughly a 50-50 partnership where human judgment, style, and accountability return to the foreground. Progress unfolds in a Blue â†’ Silver â†’ Gold cadence: rough notions are sketched in minutes (Blue), refined into serviceable drafts (Silver), and polished into publication-grade artifacts (Gold)â€”much like a film studio that begins with storyboards, advances to rehearsed scenes, and ends with the final cut for theaters.

FutureSight therefore acts as the compass that orients the entire exocortex roadmap. By scoring thousands of community-submitted predictions about emergent protein families, solvent regimes, and assay economics, the contest yields a probabilistic timeline of when each breakthrough is most likely to occur. Those calibrated forecasts flow straight into the exocortexâ€™s scheduler: agent clusters that would otherwise idle on generic literature synthesis are redeployed to higher-value jobsâ€”enumerating sequence libraries most likely to survive extreme heat or simulating folding under chaotropic shocks. Because capital, compute, and robotic bench time are now allocated in proportion to forecasted payoff, the design-build-test loop on thermostability compresses from months to days. Status is now tracked along three clearly separated layers: 1) Blue/Silver/Gold traffic lights signal project-level risk and determine whether a workstream is funded, paused, or sunset. 2) A live KPI â€œaltimeterâ€ shows continuous, numeric readoutsâ€”median melting temperature, cost-per-variant, purification throughputâ€”that rise or fall as agents generate new data. 3) Within that KPI stack, discrete checkpoints are divided into programme-wide Milestones (e.g., â€œProtein retains 50 % activity at â‰¥ 140 Â°Câ€, â€œAssay cost < $0.01 per variantâ€, â€œField-deployable purification achievedâ€) and fine-grained Badges that tag individual constructs (â€œHeat-Survivorâ€ â‰¥ 120 Â°C stability, â€œChaotrope-Resilientâ€ folds in â‰¥ 6 M GuHCl, â€œNano-Batch-Readyâ€ runs in 5 ÂµL reactions). In short, sharper foresight â†’ sharper prioritisation â†’ denser experimental cadenceâ€”transforming what was once a sprawling moonshot into a tightly coupled sprint toward the protein-thermostability stress test.

Anchor your mental legend nowâ€”the deep dive on the â€œMilestone Ladderâ€ arrives in Â§5, but keep this laminated quick-reference card in view (Rivera-credit mechanics in the side-note):  

Milestone Ladder â€” programme-wide (Capitalised)  
â€¢ Blue: prototype dog-fooded by core team  
â€¢ Silver: internally validated across two departments  
â€¢ Gold: externally benchmarked against state-of-the-art baselines  
â€¢ Platinum: field-grade, regulatory-ready  

Module badges â€” per-feature (lower-case)  
â€¢ bronze: smoke tests pass  
â€¢ silver: p95 latency â‰¤ 250 ms sustained  
â€¢ gold: â‰¥ 95 % semantic faithfulness  
â€¢ platinum: peer-reviewed â€œnovel-insightâ€ citation  

Gatekeeper metrics â€” always-on  
â€¢ uptime â‰¥ 95 %  
â€¢ audit-trace depth â‰¥ 3 hops  
â€¢ peer-impact score â‰¥ 0.70  

KPI â†’ Ladder unlocks  
â€¢ 10Ã— drafting speed-up â†’ Blue  
â€¢ â€œWhite-spaceâ€ alerts fire within 24 h for three consecutive sprints â†’ Silver  
â€¢ 2Ã— cross-lab co-authorship edges â†’ Gold  

Timeline  
â€¢ Quiet-Blue build: Q4 2024  
â€¢ Public-Blue pilot: early 2025  
â€¢ Forecast-Contest debut: Blue, sprinting to mid-Silver within 12 months while pocketing latency, faithfulness, and novelty badges.  

Legend secured; onward to the build.  

(Side-note: Rivera-credits are minted only when GPU compute-hours, reagent lots, or accredited human-review slots are escrow-locked at spot-price parity; any holder can redeem 1 credit for an equivalent basket. A Â±2 % peg-band auto-triggers treasury open-market ops, and quarterly fee-funded burns drain excess supplyâ€”keeping inflation and cross-market stability bounded.)

Road-map Cheat Sheet. The Blue contestâ€”publicly codenamed â€œFutureSightâ€â€”serves both as a stand-alone benchmark and as the inaugural live-telemetry stream for the larger exocortex effort, ensuring that insights from day-one experiments flow unbroken to decade-scale ambitions. All activity is fuelled by Rivera-credits, a programmable cryptographic chit redeemable one-to-one for GPU minutes, wet-lab reagents, or expert-review bandwidth; by collapsing three historically siloed budgets into a single liquid token, Rivera-credits let discoveries trade resources in real time and vaporize month-long grant-cycle friction. Advancement within FutureSight follows a three-step ladder: 1) Blue badgeâ€”any module that ingests, enriches, and openly annotates data while sustaining â‰¥95 % availability (our opening SLA); 2) Silver badgeâ€”prototypes whose reasoning pathways remain fully reproducible, evidenced by trace logs that withstand ten independent double-blind audits; 3) Gold badgeâ€”human-in-the-loop deployments that culminate in at least one peer-reviewed paper explicitly crediting the agent (paid for in Rivera-credits, naturally). Progress is quantified by three headline KPIs: (i) a 10Ã— reduction in time from raw lab notes to citable draft, (ii) self-refreshing knowledge graphs that flag unstudied â€œwhite-spaceâ€ questions within 24 h of data ingestion, and (iii) agent-curated matchmaking that doubles the annual rate of cross-disciplinary collaborations at participating labs. These targets anchor a fixed timelineâ€”2025 pilot â†’ 2027 scale-up â†’ 2029 cross-field synthesis â†’ 2031 federated exocortex beta â†’ 2033 adaptive orchestration â†’ 2035 ubiquitous lab adoption. Every milestone is bound by a commit-then-reveal protocol: forecasts are cryptographically sealed (staking Rivera-credits as collateral) and later unsealed at immutable checkpoints, baking accountabilityâ€”and a self-refilling pool of shared resourcesâ€”into the very architecture of progress.

From 2025 to 2035, the FutureSight forecasting contest inaugurates the â€œBlueâ€ checkpoint on the exocortex roadmapâ€”shared cognition at cloud speed. Instead of rehashing implants versus wearables, imagine an exocortex as a browser tab that finishes your thoughts before you reach the period. The real breakthrough is the Rivera-credit micro-economy threaded through every prediction, citation, and code commit. Each submission is hashed on-chain; if the forecast later ranks in the top decile of Brier scoreâ€”or if peer review flags it as â€œexceptionally arguedâ€â€”the smart contract mints Rivera-credits tied to the authorâ€™s DID. Credits circulate like carbon offsets: Lab A, for instance, earns 120 for open-sourcing a docking kernel, sells 40 on the spot market to buy wet-lab consumables, stakes 50 to reserve 600 GPU-hours, and donates the rest to a meta-analysis bounty. Dynamic reputation badges (â€œstability,â€ â€œreproducibility,â€ â€œserendipityâ€) update a public talent graph that hiring committees and grant panels query in real time. Because compute, reagents, telescope time, and even human review bandwidth are all priced in the same token, the market continuously reallocates scarce resources toward hypotheses with the highest expected information gain. Interaction traces from this Blue cohortâ€”credit flows, skill embeddings, bottleneck metricsâ€”seed the Silver stage (2027â€“2029) and ultimately the Gold era (2030â€“2035), where autonomous agent colonies arbitrage Rivera markets on their own behalf. FutureSight is less a contest than the ignition of an economic substrate for partially self-driving discovery.

To orient the reader: throughout the roadmap we write Milestones with a capital â€œMâ€ to denote consortium-wide, time-gated inflection points (e.g., â€œM3: 1 k stable-fold libraryâ€) whereas lower-case badges are the fine-grained, automatically-awarded micro-achievements an agent or lab earns on the way (â€œstability-badge-36 Â°Câ€, â€œclean-code-badgeâ€). Hitting a badge mints a trickle of Rivera-creditsâ€”our internal unit of account that simultaneously purchases GPU cycles, wet-lab queue time, and peer-review bandwidth. Ten percent of every badge payout is automatically routed to upstream dependencies, ensuring that foundational tools continue to accrue resources; Milestone completions, by contrast, unlock a much larger, one-off tranche from the central treasury and reset the badge exchange rate. The whole economy is benchmarked against a single universal proxy task: quantitative improvement in protein thermostability. We chose thermostability because it is inexpensive to assay at scale, tightly correlates with expression yield and functional robustness across diverse enzyme families, andâ€”cruciallyâ€”maps cleanly onto both simulation and experimental pipelines. With this scaffold in mind, the subsequent Dr. Rivera vignette should feel less like science fiction and more like a walk-through of the credit flows and performance signals the roadmap promises.

At its core, the science exocortex we envision is an AI co-researcherâ€”an external, networked â€œcognitive lobeâ€ that listens to your questions, surfaces relevant facts, runs its own chains of reasoning, and then converses with you as a trusted lab partner. We have already field-tested four proof-of-concept modules, each tuned to one of FutureSightâ€™s three target signals and benchmarked by two internal metrics: an ontology-merge score (e.g., 90 % of 20 000 MeSHâ†”PACS term pairs correctly aligned after each nightly retraining) and a hypothesis-retention half-lifeâ€”the time for the agentâ€™s rank-weighted belief in a new hypothesis to decay to 50 % as fresh evidence accumulates (currently â‰ˆ12 days). The modules are: (1) Preprint Horizon, which streams the latest arXiv uploads and flags ones germane to your work, directly feeding the preprints signal; (2) ClusterScope, which reclusters the entire citation graph every night to reveal nascent topical clusters before they become obvious; (3) BridgeFinder, which traces concept-level paths between otherwise disjoint clusters, surfacing high-leverage bridges; and (4) CommitSeal, which records every autonomous action with a cryptographic commit-then-reveal, preserving a tamper-proof audit trail for all detected preprints, clusters, and bridges alike. Colour-coded interaction phases keep collaboration legible: Blue sessions are exploratory, Silver sessions synthesize and critique, and Gold sessions govern high-stakes movesâ€”submissions, experimental runs, or funding decisionsâ€”where both human and exocortex place their reputations on the line.

FutureSight welds its colour-coded Milestone ladder (Blue â†’ Indigo) to two hard, auditable gates and overlays an orthogonal â€œAudit Metalâ€ badge that rates predictive lift. A five-member Steering Councilâ€”three rotating domain fellows, one ethicist, one industry observerâ€”must ratify every Milestone jump, but the vote is automatic when metrics clear the bar. To climb from Blue to Silver-Blue a project must (i) achieve â‰¥0.90 ontology-merge on the open-source ConceptFusion benchmark and (ii) demonstrate a hypothesis-retention half-life of â‰¥18 months under continual-learning stress. Example walk-through: the 2026 â€œSelf-Healing Polymer Loopâ€ entered at Blue with a 0.71 merge score and 9-month half-life. After six refinement sprintsâ€”deduplicating predicates, importing materials-science triples, retraining its agentâ€”it reached 0.93 and 20 months. The Councilâ€™s sign-off was a formality: the project advanced to Silver-Blue and earned its first Audit Metal for a +27 % prediction gain. Badges are re-audited every six months on a public leaderboard (Silver â‰¥ 25 % gain, Gold â‰¥ 40 %, Platinum = â‰¥ 2 pp self-improvement per quarter), while Milestones keep the colour scale; every dashboard reminds users that metals and colours are independent. By 2028 FutureSight targets 50 Blue-or-higher projects across five disciplines, at least ten of which reach Indigo autonomyâ€”running an end-to-end hypothesisâ†’testâ†’publication loop for a full fiscal quarter with no human prompts and cutting the cycle time eight-fold.

Every item on the ledger reduces to three primitives: a cognition node (an instance of human or AI reasoning), a data node (a block of measured evidence), and a control edge (the logic-bearing link that joins them). With that foundation, colour now encodes two orthogonal grammars. The Milestone paletteâ€”capitalised Blue, Silver, and Goldâ€”marks fixed three-year capability gates along the long-range roadmap. Nested inside those gates, the audit-trail palette tags the primitives themselves: warm reds-to-oranges for writable nodes, cool blues for provisional edges, and green-to-grey for edges that have passed validation. This inner layer lets analysts track day-to-day evidence flow without blurring it with macro Milestone status. Capitalised Milestone colours thus coexist with lower-case blue/silver/gold badges that function as rolling score-cards and are re-issued each July. On 12 July, for example, the Gen-IV Battery programme uploads new test data, spawning an orange data node. The validator logs an 8 % gain in Wh kgâ»Â¹ versus the 2024 baseline, draws a green control edge to the teamâ€™s existing silver badge, andâ€”because the gain exceeds the 5 % progressive-delta thresholdâ€”seals the edge into grey. The attached smart contract then mints one Rivera-credit and deposits it in the programmeâ€™s carbon-offset wallet. By keeping these short-cycle badge events distinct from the overarching Silver Milestone (which governs the 2024â€“26 build permit), the system preserves apples-to-apples statistics no matter where a project sits on the Milestone ladder.

Entering the Gold phase, cross-team retrospectives crowned protein thermostabilityâ€”quantified by a one-minute fluorescence Î”Tm sweepâ€”as the unifying stress-test that cleanly bridges simulation and bench; Riveraâ€™s high-throughput enzyme-repair program, already cycling weekly between in-silico design and 96-well validation, thus became the flagship sandbox. To keep this benchmark inseparable from every upstream decision, we encode the entire workflow in a minimalist, self-auditing colour ledger. Only three chromatic primitives survive: warm tones for cognition, cool tones for data, and a green-to-grey ramp for control transfers. Each hue binds to a single ledger primitive: warm nodes denote autonomous reasoning that earns no credits, cool nodes log passive data touch-points, and every green edge that terminates in a grey human-validation box auto-calls a smart-contract payout of one Rivera-credit (â‰ˆ 1 USD). Any attempt to shortcut the pipelineâ€”claiming foresight without human sign-off or bypassing the mandated Î”Tm projectionâ€”scrambles the colour checksum and voids the run. The visual grammar is more than aesthetic; it doubles as an on-screen audit trail that streams four early-warning metricsâ€”nDCGÏ€ (literature-relevance lift), Î”SimRMSE (simulation error delta), KLdrift (distribution drift), and LatencyCV (workflow jitter)â€”each with a field-tested safety gate first detailed in Â§3.2: (1) nDCGÏ€ > 0.90 (Ï€-weighted normalized DCG tuned on five years of citation lag), (2) Î”SimRMSE < 0.05 Â°C (improvement in simulation RMSE empirically tied to â‰¥ 95 % wet-lab concordance), (3) KLdrift < 0.10 bit (Kullbackâ€“Leibler divergence between predicted and observed Î”Tm distributions), and (4) LatencyCV < 0.15 (coefficient of variation in loop turnaround time). Crossing any threshold flashes the offending node crimson, supplying a real-time, colour-coded fuse between bibliometric uplift and lab reality.

Bottom line first: Riveraâ€™s exocortex-guided folding hunch is trading at $0.72 on the internal prediction marketâ€”comfortably above the programmeâ€™s $0.70 funding barâ€”even after the scoring engine discounts it for novelty risk and recency bias. The roadmap stays colour-coded and crisp: Blue (2025â€“2026) builds and benchmarks single-agent exocortex primitives; Silver (2027â€“2028) weaves those primitives into reliable multi-agent workflows that can run full research sprints under light human oversight; Gold (2029â€“2030) scales, audits, and open-sources the mature system, turning it into a safety-vetted, globally accessible co-researcher. Protein thermostability is picked as the stress-test phenotype because it marries atomic-scale complexity with a one-number read-out (Î”Tâ‚˜), enabling cheap, quantitative grading of both idea quality and wet-lab execution across many sites. During Gold, the agent swarm designs a metalloprotein predicted to hold its fold 20 Â°C above wild-type, preregisters the protocol, and wagers 240 market credits. Riveraâ€™s dashboard then surfaces four plain-language early-warning signals that together forecast replication success: (1) nDCGÏ€ (â€œnormalized Discounted Cumulative Gain with priorsâ€)â€”how high the true hits rank in the AIâ€™s candidate list; (2) Î”SimRMSE (â€œdelta-simulation root-mean-square errorâ€)â€”the average gap between in-silico forecasts and bench data; (3) kNN-Jaccardâ€”how confidently nearest-neighbour sequences consign the design to a stable-fold family; and (4) p_predâ€”the agentâ€™s own calibrated probability of success. The underlying logistic model freezes literature priors at a January-2024 arXiv snapshot, refreshes simulations nightly on 1 000 A100 GPUs, streams live assay deltas from 37 partner bioreactors, and recomputes neighbourhood stats every Sunday against the latest UniProt release. Back-tests on 14 000 protein designs show that when nDCGÏ€ > 0.9, Î”SimRMSE < 0.2, kNN-Jaccard falls in the 0.4â€“0.7 novelty band, and p_pred > 0.60, 85 % of claims replicate within error barsâ€”turning the quartet into a gestalt safety gauge rather than alphabet soup. A post-review market price of $0.72 automatically triggers a $600 k rapid-execution grant for synthesis and assays; by mid-2031, three independent labs confirm a median 23 Â°C stability boost, settling the market at $0.97 and converting Riveraâ€™s credits into a $4 M follow-on award plus priority compute hours. At Goldâ€™s close, each pioneer uploads a cryptographically time-stamped â€œ2030 Outcome Cardâ€ linking forecasts to replications; the cards are resurfaced in 2035 for scoring, and meeting the benchmark unlocks further fundingâ€”closing the loop between visionary bets and measurable impact. Thus the Blue-Silver-Gold arcâ€”build, integrate, operationaliseâ€”roots every vignette in the 2025â€“2030 timeline while aligning individual stories with the programmeâ€™s longitudinal goals.

Consider Dr. Rivera, who wakes up with a hunch: â€œchirped laser fields could accelerate protein folding in vivo.â€ She drops a two-page sketch into FutureSightâ€™s Blue inboxâ€”a free-form note meant to trap the spark before bureaucracy creeps in. The platform parses the note into a knowledge graph and lights up two panels of dials, each with a one-line gloss before any math appears in the appendix.  

Scientific Evidence Dials  
â€¢ Priors (nDCGÏ€): â€œGiven what science already knows, how surprising yet still plausible is this idea?â€  
â€¢ Sim Evidence (Î”SimRMSE): â€œDo quick-and-dirty GPU folding runs trend in the same direction?â€  
â€¢ Literature Analogues (kNN-Jaccard): â€œHas anything even remotely like this succeeded before?â€  
â€¢ Crowd Forecast (p_pred): â€œWould informed peers stake reputation on replication and impact?â€  

Policy Dials (the same three badge modifiers introduced in Â§3)  
â€¢ Originality Badge (Î»_FO): boosts ideas that open entirely new search spaceâ€”+10 % to R for every Ïƒ above the novelty baseline.  
â€¢ Trend Penalty Badge (Î»_TP): tempers me-too proposalsâ€”0.85 Ã— R if the idea lands in the top decile of over-published keywords.  
â€¢ Overclaim Risk Badge (Î»_OR): safeguards against hypeâ€”R is multiplied by (1 â€“ expected exaggeration rate), estimated from Riveraâ€™s text features and past lab press releases.  

Under the hood, a ridge-regularized logistic modelâ€”retrained weekly on 48 000 historical hypothesesâ€”maps the four evidence signals into a raw â€œReplication Likelihoodâ€ score: Râ‚€ = Ïƒ(âˆ‘ wáµ¢ záµ¢). For Riveraâ€™s upload, the platform returns Priors = 0.67, Sim = 0.58, Literature = 0.61, Crowd = 0.62, yielding Râ‚€ = 0.74. The policy layer then applies Î»_FO = 1.07, Î»_TP = 0.93, Î»_OR = 0.95, producing a final R = 0.74 Ã— 1.07 Ã— 0.93 Ã— 0.95 â‰ˆ 0.70â€”still a strong green light, but slightly tempered for fashion-risk and rhetorical flourish. All published coefficients, market prices, and dial settings are released with Â±0.03 Laplace noise; because any single hypothesis can shift a bounded-sensitivity score by â‰¤ 0.03, this guarantees Ïµ â‰ˆ 1 differential privacy while leaving the rank ordering virtually unchanged for decision-making.

At a glance, every contribution still passes five clearly marked stationsâ€”Encrypt, Score, Forecast, Review, Revealâ€”and the rules behind each are now glass-clear. Concretely, each submission earns a provisional composite at two temporal snapshots (upload-day and quarterly refresh), built from four headline metricsâ€”precision, novelty, methodological rigor, and citation-context fit (formulas in Appendix A). Why test with â€œphantomâ€ abstracts? Because asking a modelâ€”or a humanâ€”to outline a study that has not yet been done forces it to commit to methods, baselines, and results that the future record will either confirm or falsify; the gap between imagined and eventual reality becomes a measurable yardstick of foresight rather than rhetoric. For example, an abstract drafted in mid-2025 might predict that â€œa 128-layer diffusion transformer fine-tuned on single-cell RNA graphs will halve wet-lab screening costs by Q4 2027â€â€”a claim we can check against actual publications and cost data when that quarter arrives. We first veil every raw metric with Â±0.03 Laplace noise so trends surface while private reference sets stay submerged. Three policy dials then reshape that noisy base: ForesightOriginality rewards forward-leaning citations, TrendPenalty trims me-too heat, and OverclaimRisk tethers headline bravado. A raw 0.70 can thus glide to 0.78, skid to 0.69, and settle at a publishable 0.64. (1) Encrypt: the upload is hashed, AES-256 sealed, and its key Shamir-split across five neutral vaults run by the Open Science Trust. (2) Score: an append-only ledger stores the cipher while LLM and citation-graph heuristics mint the Automated Score. (3) Forecast: that score seeds a permissionless prediction market where academic exchanges keep spreads tight and bad actors thin. (4) Review: once per quarter, a double-blind Silver Round of domain experts refines the evaluation without seeing author names or market prices. (5) Reveal: after a 12â€“18-month timelock, a smart contract reunites the key shards, publishes the full work, and triggers a Gold Round audit that folds in citations, replications, and market history to mint a definitive quality badgeâ€”while stake-slashing and the public trail deter collusion.

Benchmark at a Glance. The life-cycle of every entry now unfolds in six crisp steps: (1) Lock-inâ€”teams encrypt their forecasts and hand them to a key-escrow service that time-stamps the ciphertext, guaranteeing no back-dating or snooping; (2) Silver checkâ€”once the corresponding week of arXiv preprints appears, the bundle is decrypted and scored on four core metrics (Precision@20, Recall@100, nDCG, Semantic Diversity); (3) Trend pulseâ€”each topicâ€™s weekly-volume slope (Î”trend) is compared with its 90-day mean (Î¼â‚‰â‚€d) so that Î”trend > Î¼â‚‰â‚€d flags an â€œacceleratingâ€ hot spot; (4) Gold checkâ€”after peer review has winnowed the literature, the identical metrics are recomputed on this matured snapshot, letting us see how much early â€œsilverâ€ insight survived real scrutiny; (5) Noise shieldâ€”all counts are dithered with mean-zero Laplace noise (Îµâ‰ˆ1) to blunt overfitting; (6) Composite scoreâ€”the four z-normalized core metrics are adjusted by ForesightOriginality, TrendPenalty, FeasibilityBoost, and EthicsPenalty, so a public gain of +0.10 truly means one-tenth of a sigma in holistic scientific foresight.

Peer review gets messy when forecasts are visible: pundits can copy winners, collaborators can back-channel, and honest referees may reveal trade secrets. Our remedy is a two-layer disclosure protocol. Each teamâ€™s full belief vector is sealed in a cryptographic locker, while the public ledger sees only a â€œsurprise termâ€ â€“ the zero-mean residual left after a Kalman update. In plain English: we publish the noise, not the signal, so sensitive priors stay hidden yet performance can still be audited. The workflow unfolds like this. 1) Researchers post concise project briefsâ€”hypotheses, protocols, deliverablesâ€”on the open Idea Bazaar. 2) An LLM-triage service clusters briefs and dispatches them to automated critics plus gig-economy reviewers; their weighted opinions fuse into a provisional Silver score that also seeds a prediction-market contract. 3) Domain-specific agent squads draft detailed methods, secure datasets, and spin up lab-on-chip or cloud experiments, pushing the Silver score toward convergence with every verified milestone. 4) When wet-lab data, code, or replication logs are notarized on-chain, an oracle promotes the claim to Gold and settles the market, paying those who were early and right. 5) Rewards ripple through reputation vectors for every human or AI contributor, sharpening future matchmaking, funding, and reviewer selection. 6) Finally, the locker opens just long enough for the private prior and the observed outcome to be combined, after which only the decorrelated residual (historical accuracy râ‰ˆ0.87 on pilot datasets) is written to the ledger. Technical proofs and full covariance derivations live in Appendix A.

Since the Blue / Silver / Gold cadence became the communityâ€™s metronome, debate has shifted from possibility to performance. Beneath the ritual, every manuscript is represented as a hidden â€œmerit gradientâ€ gâ‚€. Each reviewer contributes a noisy glimpse gáµ¢. A one-step Kalman update forms the crowdâ€™s best guess Åáµ¢, then writes only the decorrelated residual Î· = gáµ¢ â€“ Åáµ¢ to the public ledger. Why is this safe to reveal? In the linear-Gaussian setting, gâ‚€ and every gáµ¢ are assumed jointly normal. The Kalman gain K is chosen so that Åáµ¢ is the minimum-variance unbiased estimate of gâ‚€, forcing Cov(Î·, gâ‚€) = 0 and Cov(Î·, gâ±¼) = 0 for all reviewers j. In essence, we publish only the â€œnoiseâ€ so no useful information can leak. Put plainly, after we subtract â€œwhat the crowd already knows,â€ what remains is pure randomnessâ€”no pattern an adversary can latch onto, no breadcrumbs back to confidential text or reviewer identity. During an 18-month pilot across a dozen universities and thousands of sealed markets, the math held up: zero AES shards leaked, no differential-privacy guard-rail ever twitched, and red-team extractions stayed below 0.1 %. Calibration remained sharpâ€”provisional Silver scores predicted eventual Gold at r = 0.87â€”and market makers reclaimed virtually all float. Teams iterated at breakneck speed: median sandbox-to-resubmission time was 11 minutes on barely half the allocated GPU budget. Even with a 30 % Gaussian blur on reviewer input, the system flagged plagiarism clones and trend-chasing drafts with a 92 % precision far beyond traditional peer review. What began as a speculative safeguard now stands as an empirically proven accelerator for disciplined, crowd-audited discovery.

Building on the empirical wins of HLM-Cite (72 Â± 2 % of new citation edges correctly forecast in a 12-month hold-out) and R&D-Agent (autonomously reproduced and improved 18 / 30 ML benchmarks within one week), we now usher every manuscript through a colour-coded, time-stamped gauntlet: blue, silver, then gold. Blue scores appear instantly (t â‰ˆ 0): an ensemble of language-model predictors emits an automated â€œblueâ€ gâ‚€, unlocking $2 k micro-grants for the most promising 10 %. During the next three years, at three-month cadence (t = 3, 6, â€¦ , 36 mo), the work perches on the â€œsilver board.â€ We suppress the ground-truth reviewer consensus gáµ¢ but reveal a noisy proxy Åáµ¢ = gáµ¢ + Î·, Î· ~ ğ’©(0, 0.3 gáµ¢) freshly resampled each quarter. An LMSR market (b = 500) pays $1 only if the paper ultimately lands in the top decile of the forthcoming â€œgold board,â€ forcing traders to price P(top-10 % | Å-history) rather than gáµ¢ itselfâ€”an ill-posed deconvolution that blunts gaming. Twelve to eighteen months post-acceptance (t â‰ˆ 12â€“18 mo) replication packets are adjudicated, the noise curtain lifts, and definitive gold rankings emerge: 40 % of the pool flows to the authors for follow-on work, 40 % to accurate forecasters, and 20 % seeds the next blue cycle, rendering the system fiscally self-propelling. In simulation, a public-only Kalman filter recovers gáµ¢ with RMSE = 0.19â€”too crude for leakageâ€”while the market, knitting heterogeneous priors, achieves RMSE = 0.07. Cloaked in Îµ = 1, Î´ â‰ˆ 10â»âµ differential privacy on similarity queries, this blue-silver-gold cadence forms a live, incentive-aligned compass; the replication-validated gold scores feed back into HLM-Citeâ€™s predictors and R&D-Agentâ€™s search heuristics, so each funded experiment retrains the models that launch the next round, completing the ideation â†’ funding â†’ experiment â†’ evaluation loop at the very heart of our benchmark cycle.

Concretely, the loop now operates as follows: an ideation agent proposes a slate of hypotheses, each embedded in vector space and automatically cross-referenced with both published papers and a cryptographically escrowed vault of in-press manuscripts. Authors volunteer these embargoed drafts because they receive priority timestamps, pre-publication citation credits, and governance tokens redeemable at journal release; a lightweight smart-contract NDA keeps the full text encrypted until the time-lock lapses. HLM-Cite shows that even on this partially revealed content the LLM layer can attribute citations, estimate novelty, and issue a first-pass â€œblueâ€ score in seconds, as only redacted embeddings ever exit the secure enclaveâ€”guarding against premature leaks. The candidates then flow into a scoring layer where crowd-forecasting plus automated back-testing yield a refined â€œsilverâ€ score that fuses probabilistic foresight with mechanistic plausibility. Top-ranked ideas are automatically packaged into mini-grant proposals, and a tokenised funding pool releases capital proportionally to these scoresâ€”an approach already validated by the on-chain micro-funding experiments reported in R&D-Agent. Once funded, robotic labs or cloud simulators run the prescribed experiments, streaming back structured results that close the loop. The final â€œgoldâ€ score is a weighted aggregate of (i) foresight accuracy (did predictors anticipate the outcome?), (ii) empirical lift (did the experiment confirm the key claim?), and (iii) market calibration (did real adoption or follow-on funding materialise?). Because HLM-Cite covers the ideaâ†’scoring hand-off and R&D-Agent covers the fundingâ†’experiment hand-off, every leg of this cycle has a working prototype today. Taken together, cryptographic vaults guarantee trusted data flow, prediction markets convert insight into scarce capital, multi-tier metrics keep everyone honest, and exocortex agents orchestrate the entire pipelineâ€”delivering a fully automated, end-to-end research flywheel that is measurable, investable, and imminent.

Existing open-domain â€œfuture-forecastingâ€ leaderboards have proved too easy to game: models can scrape leaked drafts, overfit to well-known trend curves, or impress judges with speculative rhetoric that can never be falsified, so reported gains often reflect information advantage rather than genuine foresight skill. To remove this hindsight contamination, we propose time-locking the evaluation setâ€”hundreds of in-press manuscripts volunteered by their authorsâ€”inside a cryptographically sealed vault until the scoring date. Because no publicly available corpus contains these embargoed ideas, any system that predicts their abstracts, keywords, or citation graph must truly extrapolate beyond its training distribution rather than regurgitate memorized text. Our design is inspired by earlier, smaller-scale pilots such as the arXiv Early-Access and IARPA FOCUS forecasting challenges, which showed that withholding preprints for even a few weeks dramatically sharpened the signal-to-noise ratio of foresight metrics.
 
