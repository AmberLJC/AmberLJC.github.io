<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>From 0→∞: My PhD Lessons on Innovating User-Centric ML Systems</title>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-T1JYF39V5T"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-T1JYF39V5T');
    </script>
    
    <!-- GoatCounter Analytics -->
    <script data-goatcounter="https://amberljc.goatcounter.com/count"
            async src="//gc.zgo.at/count.js"></script>
    
    <style>
        :root {
            --primary-color: #37352f;
            --secondary-color: #5f6b7a;
            --accent-color: #2e75cc;
            --text-color: #37352f;
            --background-color: #ffffff;
            --section-bg: #fafafa;
            --border-color: #e1e5e9;
            --light-text: #787774;
            --code-bg: #f7f6f3;
            --quote-bg: #f1f1ef;
            --hover-bg: #f7f6f3;
        }

        * {
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Helvetica Neue', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 0 24px;
            color: var(--text-color);
            background-color: var(--background-color);
            margin-top: 80px;
            font-size: 16px;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        .topnav {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            background-color: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-bottom: 1px solid var(--border-color);
            box-shadow: 0 1px 3px rgba(0,0,0,0.05);
            overflow: hidden;
            z-index: 1000;
            padding: 0;
            margin: 0;
        }

        .topnav a {
            float: left;
            display: block;
            color: var(--primary-color);
            text-align: center;
            padding: 16px 20px;
            text-decoration: none;
            font-size: 15px;
            font-weight: 500;
            transition: background-color 0.2s ease;
            border-radius: 6px;
            margin: 8px 4px;
        }

        .topnav a:hover {
            background-color: var(--hover-bg);
            color: var(--primary-color);
        }

        .topnav a.active {
            background-color: var(--accent-color);
            color: white;
        }

        h1, h2, h3, h4 {
            color: var(--primary-color);
            margin-top: 1.8em;
            margin-bottom: 0.8em;
            font-weight: 600;
            line-height: 1.3;
        }

        h1 {
            font-size: 2.2em;
            border-bottom: none;
            padding-bottom: 0;
            margin-bottom: 0.5em;
            text-align: left;
            font-weight: 700;
        }

        h2 {
            font-size: 1.6em;
            border-left: none;
            padding-left: 0;
            margin-top: 2.2em;
            margin-bottom: 1em;
            font-weight: 600;
        }

        h3 {
            font-size: 1.3em;
            color: var(--primary-color);
            margin-top: 1.8em;
            margin-bottom: 0.8em;
        }

        h4 {
            font-size: 1.1em;
            color: var(--primary-color);
            margin-top: 1.5em;
            margin-bottom: 0.6em;
        }

        p {
            margin-bottom: 1.2em;
            font-size: 16px;
            line-height: 1.7;
            color: var(--text-color);
        }

        .citation {
            color: var(--light-text);
            font-size: 14px;
            background-color: var(--quote-bg);
            padding: 24px;
            border-radius: 12px;
            margin: 32px 0;
            border: 1px solid var(--border-color);
            box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        }

        .citation-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 16px;
            padding-bottom: 12px;
            border-bottom: 1px solid var(--border-color);
        }

        .citation-header h3 {
            margin: 0;
            font-size: 1.1em;
            color: var(--primary-color);
        }

        .header-icon {
            margin-right: 8px;
            vertical-align: middle;
        }

        .btn-icon {
            margin-right: 6px;
            vertical-align: middle;
        }

        .copy-btn {
            background-color: var(--accent-color);
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 6px;
            font-size: 13px;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s ease;
            box-shadow: 0 1px 2px rgba(0,0,0,0.1);
            display: flex;
            align-items: center;
        }

        .copy-btn:hover {
            background-color: #1e5aa8;
            transform: translateY(-1px);
            box-shadow: 0 2px 4px rgba(0,0,0,0.15);
        }

        .copy-btn:active {
            transform: translateY(0);
        }



        #citation-text {
            background-color: var(--code-bg);
            padding: 16px;
            border-radius: 8px;
            border: 1px solid var(--border-color);
            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.5;
            overflow-x: auto;
            margin: 0;
        }

        ul, ol {
            margin-bottom: 1.2em;
            padding-left: 1.5em;
        }

        li {
            margin-bottom: 0.6em;
            line-height: 1.6;
            color: var(--text-color);
        }

        .highlight {
            background-color: var(--quote-bg);
            padding: 20px;
            border-radius: 8px;
            margin: 24px 0;
            border-left: 3px solid var(--accent-color);
        }

        .section {
            margin-top: 2em;
            background-color: var(--section-bg);
            padding: 24px;
            border-radius: 12px;
            border: 1px solid var(--border-color);
            box-shadow: 0 1px 3px rgba(0,0,0,0.05);
            transition: all 0.2s ease;
            position: relative;
        }

        .section:hover {
            transform: translateY(-1px);
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .section h2 {
            margin-top: 0;
            padding-bottom: 12px;
            border-bottom: 2px solid var(--accent-color);
            color: var(--primary-color);
        }

        .subsection {
            margin-top: 1.5em;
            padding: 20px;
            border-radius: 8px;
            background-color: var(--background-color);
            border: 1px solid var(--border-color);
            box-shadow: 0 1px 2px rgba(0,0,0,0.05);
            transition: all 0.2s ease;
            position: relative;
        }

        .subsection:hover {
            transform: translateY(-1px);
            box-shadow: 0 2px 6px rgba(0,0,0,0.08);
        }

        .subsection h3 {
            margin-top: 0;
            color: var(--primary-color);
            position: relative;
            padding-left: 12px;
        }

        .subsection h3::before {
            content: '';
            position: absolute;
            left: 0;
            top: 50%;
            transform: translateY(-50%);
            width: 4px;
            height: 20px;
            background-color: var(--accent-color);
            border-radius: 2px;
        }

        a {
            color: var(--accent-color);
            text-decoration: none;
            transition: color 0.2s ease;
            border-bottom: 1px solid transparent;
        }

        a:hover {
            color: var(--accent-color);
            border-bottom: 1px solid var(--accent-color);
        }

        strong {
            color: var(--primary-color);
            font-weight: 600;
        }

        em {
            color: var(--secondary-color);
            font-style: normal;
            font-weight: 500;
        }

        blockquote {
            margin: 1.5em 0;
            padding: 0 1em;
            border-left: 3px solid var(--accent-color);
            background-color: var(--quote-bg);
            border-radius: 0 6px 6px 0;
        }

        blockquote p {
            margin: 1em 0;
            font-style: italic;
            color: var(--secondary-color);
        }

        code {
            background-color: var(--code-bg);
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
            font-size: 14px;
            color: var(--primary-color);
        }

        pre {
            background-color: var(--code-bg);
            padding: 16px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5em 0;
            border: 1px solid var(--border-color);
        }

        pre code {
            background-color: transparent;
            padding: 0;
            border-radius: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            border-radius: 8px;
            overflow: hidden;
            border: 1px solid var(--border-color);
        }

        th {
            background-color: var(--section-bg);
            color: var(--primary-color);
            font-weight: 600;
            text-align: left;
            padding: 12px 16px;
            border-bottom: 1px solid var(--border-color);
        }

        td {
            padding: 12px 16px;
            border-bottom: 1px solid var(--border-color);
            background-color: white;
        }

        tr:last-child td {
            border-bottom: none;
        }

        figure {
            margin: 2em 0;
            text-align: center;
        }

        figure img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        figcaption {
            text-align: center;
            color: var(--light-text);
            font-size: 14px;
            margin-top: 0.8em;
            font-style: italic;
        }

        .blog-meta {
            color: var(--light-text);
            font-size: 14px;
            margin-bottom: 2.5em;
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding-bottom: 1.5em;
            border-bottom: 1px solid var(--border-color);
        }

        .blog-meta p {
            margin: 0;
        }

        .toc {
            font-size: 15px;
            line-height: 1.5;
            margin: 2em 0;
            background-color: var(--section-bg);
            padding: 24px;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        .toc h2 {
            font-size: 1.3em;
            margin-bottom: 1em;
            margin-top: 0;
        }

        .toc ul {
            margin-bottom: 0.8em;
            padding-left: 1.2em;
        }

        .toc li {
            margin-bottom: 0.4em;
            line-height: 1.4;
        }

        .follow-btn {
            display: inline-flex;
            align-items: center;
            margin: 0 4px;
            padding: 8px 16px;
            text-decoration: none;
            border-radius: 6px;
            font-size: 13px;
            font-weight: 500;
            transition: all 0.2s ease;
            box-shadow: 0 1px 2px rgba(0,0,0,0.1);
        }

        .follow-btn:hover {
            transform: translateY(-1px);
            box-shadow: 0 2px 4px rgba(0,0,0,0.15);
        }

        .twitter-btn {
            background-color: #1da1f2;
            color: white;
        }

        .linkedin-btn {
            background-color: #0077b5;
            color: white;
        }

        @media (max-width: 768px) {
            body {
                padding: 0 16px;
                margin-top: 70px;
                font-size: 15px;
            }
            
            h1 {
                font-size: 1.9em;
            }
            
            h2 {
                font-size: 1.4em;
            }

            .blog-meta {
                flex-direction: column;
                align-items: flex-start;
                gap: 12px;
            }
            
            .follow-buttons {
                display: flex;
                gap: 8px;
            }
            
            .follow-btn {
                margin: 0;
                padding: 6px 12px;
                font-size: 12px;
            }

            .topnav a {
                padding: 12px 16px;
                font-size: 14px;
            }

            table {
                font-size: 14px;
            }

            th, td {
                padding: 8px 12px;
            }
        }

        @media (max-width: 480px) {
            body {
                padding: 0 12px;
            }
            
            .follow-buttons {
                flex-direction: column;
                gap: 6px;
            }
            
            .follow-btn {
                width: 100%;
                text-align: center;
            }
        }
    </style>
</head>
<body>

    <div class="topnav">
        <a href="../index.html">Home</a>
        <a href="../blog.html">Blog</a>
    </div>

    <h1>From 0→∞: My PhD Lessons on Innovating User-Centric ML Systems</h1>
    
    
    <div class="blog-meta">
        <p>Posted on June 15, 2025 by Jiachen Liu</p>  
         <div class="follow-buttons">
            <a href="https://x.com/JIACHENLIU8" target="_blank" class="follow-btn twitter-btn">
                <svg class="btn-icon" width="14" height="14" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"/>
                </svg>
                X
            </a>
            <a href="https://www.linkedin.com/in/jiachen-amber-liu-872506169/" target="_blank" class="follow-btn linkedin-btn">
                <svg class="btn-icon" width="14" height="14" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z" fill="currentColor"/>
                </svg>
                LinkedIn
            </a>
        </div>
    </div>

    <script>
        // GoatCounter page view display
        async function fetchGoatCounterViews() {
            try {
                // Get current date and create date range for the API call
                const startDate = '2025-01-01';
                const endDate = '2026-12-30';
                
                // Current path for matching
                const currentPath = window.location.pathname;
                console.log('Current path:', currentPath);
                
                // Fetch page views with date range
                const response = await fetch(`https://amberljc.goatcounter.com/api/v0/stats/hits?start=${startDate}&end=${endDate}`, {
                    headers: {
                        'Authorization': 'Bearer 1rhhf25kaexfg23nz6j53gfc138w4f5tpz3y9023mojfexssgie'
                    }
                });
                
                if (response.ok) {
                    const data = await response.json();
                    console.log('GoatCounter API response:', data);
                    
                    if (data.hits && data.hits.length > 0) {
                        // Find the hit that matches our current path
                        let pathHit = data.hits.find(hit => hit.path === currentPath);
                        
                        // If exact match fails, try to find a path that ends with our current path
                        if (!pathHit) {
                            pathHit = data.hits.find(hit => hit.path.endsWith(currentPath));
                        }
                        
                        // If still no match, try to find a path that contains our filename
                        if (!pathHit) {
                            const filename = currentPath.split('/').pop();
                            pathHit = data.hits.find(hit => hit.path.includes(filename));
                        }
                        
                        console.log('Found path hit:', pathHit);
                        
                        if (pathHit) {
                            const views = pathHit.count;
                            document.getElementById('page-views').textContent = views;
                            return;
                        } else {
                            console.log('No matching path found. Available paths:', data.hits.map(hit => hit.path));
                            // Show total views if specific path not found
                            const totalViews = data.hits.reduce((sum, hit) => sum + hit.count, 0);
                            document.getElementById('page-views').textContent = totalViews;
                        }
                    } else {
                        console.log('No hits data found');
                        document.getElementById('page-views').textContent = '0';
                    }
                } else {
                    console.log('API response not ok:', response.status, response.statusText);
                    document.getElementById('page-views').textContent = 'N/A';
                }
            } catch (error) {
                console.log('GoatCounter API error:', error);
                document.getElementById('page-views').textContent = 'N/A';
            }
        }
 
        // Initialize when page loads
        document.addEventListener('DOMContentLoaded', function() {
            // Small delay to ensure GoatCounter has tracked the visit
            setTimeout(fetchGoatCounterViews, 1000);
        });

        // Copy citation function
        function copyCitation() {
            const citationText = document.getElementById('citation-text').textContent;
            navigator.clipboard.writeText(citationText).then(function() {
                const btn = event.target;
                const originalText = btn.textContent;
                btn.textContent = '✅ Copied!';
                btn.style.backgroundColor = '#10b981';
                setTimeout(() => {
                    btn.textContent = originalText;
                    btn.style.backgroundColor = '';
                }, 2000);
            }).catch(function(err) {
                console.error('Could not copy text: ', err);
                // Fallback for older browsers
                const textArea = document.createElement('textarea');
                textArea.value = citationText;
                document.body.appendChild(textArea);
                textArea.select();
                document.execCommand('copy');
                document.body.removeChild(textArea);
                
                const btn = event.target;
                const originalText = btn.textContent;
                btn.textContent = '✅ Copied!';
                btn.style.backgroundColor = '#10b981';
                setTimeout(() => {
                    btn.textContent = originalText;
                    btn.style.backgroundColor = '';
                }, 2000);
            });
        }

        // Copy references function
        function copyReferences() {
            const references = document.querySelectorAll('.citation p');
            let referencesText = '';
            
            references.forEach((ref, index) => {
                referencesText += ref.textContent + '\n';
            });

            navigator.clipboard.writeText(referencesText).then(function() {
                const btn = event.target;
                const originalText = btn.textContent;
                btn.textContent = '✅ Copied!';
                btn.style.backgroundColor = '#10b981';
                setTimeout(() => {
                    btn.textContent = originalText;
                    btn.style.backgroundColor = '';
                }, 2000);
            }).catch(function(err) {
                console.error('Could not copy text: ', err);
                // Fallback for older browsers
                const textArea = document.createElement('textarea');
                textArea.value = referencesText;
                document.body.appendChild(textArea);
                textArea.select();
                document.execCommand('copy');
                document.body.removeChild(textArea);
                
                const btn = event.target;
                const originalText = btn.textContent;
                btn.textContent = '✅ Copied!';
                btn.style.backgroundColor = '#10b981';
                setTimeout(() => {
                    btn.textContent = originalText;
                    btn.style.backgroundColor = '';
                }, 2000);
            });
        }
    </script>
  
    <div class="section">
        <p>
 
    I'm super happy and grateful to share that I've recently completed my PhD! 
    Looking back, the past five years at <a href="https://symbioticlab.org">SymbioticLab</a> have been an intense mix of joy, frustration, discovery, and growth.
    As I close this chapter, I wanted to take a moment to reflect — to write down some of the lessons, thoughts, and shifts I've experienced, especially in the world of MLSys.
    This post is adapted from the conclusion section of <a href="../documents/phd-dissertation.pdf"> my thesis</a> (User-Centric Machine Learning Systems),  hopingthat they might resonate with or support future researchers walking a similar path.

        </p>
            
        <p>
    When I started my PhD in 2020, AI was largely limited to niche domains like image recognition and recommendation systems, primarily utilized by large corporations or research labs. 
    Today, in the year of 2025, it becomes a technology that seamlessly integrates into individuals' everyday life and professional workplace. 
    When AI interfaces with individuals through text, audio, images, video, and physical interactions, we need to rethink: How should we architect the new generation of ML systems to balance user-centric experiences with server-side efficiency.

        </p>
    </div>


    <div class="section toc">
        <h2>Table of Contents</h2>
        <ul>
            <li><a href="#my-thoughts">My Thoughts</a>
                <ul>
                    <li><a href="#trend-of-pervasive-ai">The Trend of Pervasive AI</a></li>
                    <li><a href="#lessons-learned">Lessons Learned for ML Systems</a>
                        <ul>
                            <li><a href="#conceptual-stages">Conceptual Stages of ML Systems Innovation</a></li>
                            <li><a href="#think-ahead">Think Ahead</a></li>
                            <li><a href="#specialized-design">Specialized System Design</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li><a href="#future-work">Future Work</a>
                <ul>
                    <li><a href="#advancing-qoe">Advancing User-Centricity and Quality of Experience (QoE) Across Diverse Modalities</a>
                        <!-- <ul>
                            <li><a href="#qoe-metrics">Defining and Optimizing Modality-Specific QoE Metrics</a></li>
                            <li><a href="#resource-management">Resource Management for Dynamic Generative Workloads</a></li>
                            <li><a href="#cross-modal">Cross-Modal QoE Synchronization</a></li>
                        </ul> -->
                    </li>
                    <li><a href="#ai-agents">AI Agents for Self-Evolving ML Systems Design</a></li>
                    <li><a href="#next-gen-agents">Next-Generation Agentic AI Systems</a></li>
                </ul>
            </li>
        </ul>
    </div>

    <div class="section" id="my-thoughts">
        <h2>My Thoughts</h2>

        <div class="subsection" id="trend-of-pervasive-ai">
            <h3>The Trend of Pervasive AI</h3>

            <p>Over the past five years, we have witnessed the rapid expansion of AI from a technology confined to specialized domains within large corporations and research labs to a tool that is increasingly integrated into the everyday life of individuals. This shift, accelerated by advancements in generative AI models such as ChatGPT, signifies a clear trend: AI is moving towards ubiquity, poised to become as commonplace in our daily existence as essential utilities like water, electricity, and the internet. With this trend, it necessitates a rethinking of its role in people's life and, consequently, how we design the underlying systems to support this integration.</p>

            <p>Looking forward, the medium through which AI delivers its value to humans will likely evolve beyond current chat-based interfaces where users proactively submit requests. Two prominent future mediums emerge:</p>

            <ol>
                <li><strong>Off-body AI:</strong> Physical robots and virtual AI agents designed to proactively complete complex tasks in the physical or digital world in an autonomous and goal-oriented manner. Their functionalities extend beyond simple command execution to complex reasoning, planning, and execution across diverse domains. Imagine an advanced cleaning robot with the ability to understand spoken commands, sophisticated vision to identify objects and a dexterous arm to manipulate items.</li>
                
                <li><strong>On-body AI:</strong> Augmented reality (AR) and virtual reality (VR) running on wearable devices such as smart glasses. This form of AI would function more like an ever-present, invisible daily assistant, subconsciously learning user habits and contexts to offer timely suggestions and support. For instance, in a professional setting, it could understand an individual's tasks and provide context for upcoming meetings, summarize discussions, or even act as a co-pilot in various professions.</li>
            </ol>
            <figure>
                <img src="figs/ai-medium.png" alt="Illustration of AI interaction mediums" style="width: 40%; max-width: 800px; margin: 2em auto; display: block; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <figcaption style="text-align: center; color: #666; font-size: 0.9em; margin-top: 1em;">Figure 1: The Future Mediums of Pervasive AI</figcaption>
            </figure>

            <p>To enable these sophisticated off-body and on-body AI experiences, a confluence of advanced technologies is necessary. These include advanced LLMs for natural language understanding and high-level planning, multimodal models for perception (e.g., vision, audio processing), reinforcement learning for dynamic decision-making in complex environments, and sophisticated control systems for robotic manipulation.</p>

            <p>These evolving interaction paradigms not only underscore the future where AI is deeply and proactively embedded in our daily activities but also directly inspire the future research directions in ML system design to support such pervasive AI, where I will discuss the underlying challenges and opportunities in the Future Work section.</p>
        </div>

        <div class="subsection" id="lessons-learned">
            <h3>Lessons Learned for ML Systems</h3>

            <p>The evolving landscape of ML underscores the critical role of ML systems, which are essential for enabling these emerging AI applications. Throughout my research journey, I continually reflect on the current ML systems landscape, asking:</p>

            <ul>
                <li><em>What kind of ML systems research deserve us to dive deeper?</em></li>
                <li><em>How do we find promising ML systems problems to solve?</em></li>
                <li><em>How should we approach these problems?</em></li>
            </ul>

            <p>Below, I share my thoughts to the questions, hoping they are helpful for future researchers. For those interested in exploring the broader landscape of ML systems research, I also maintain a curated <a href="https://github.com/AmberLJC/LLMSys-PaperList/blob/main/README.md" target="_blank">Paper List</a> that covers various aspects of generative AI systems research.</p>

            <h4 id="conceptual-stages">Conceptual Stages of ML Systems Innovation</h4>

            <p>Through my reflection of numerous ML systems projects, I've found three conceptual levels of research:</p>

            <table style="width: 100%; border-collapse: collapse; margin: 20px 0; box-shadow: 0 2px 4px rgba(0,0,0,0.1); border-radius: 8px; overflow: hidden;">
                <thead>
                    <tr style="background-color: var(--secondary-color); color: white;">
                        <th style="padding: 15px; text-align: left; font-weight: 600;">Research Phase</th>
                        <th style="padding: 15px; text-align: left; font-weight: 600;">Focus Area</th>
                        <th style="padding: 15px; text-align: left; font-weight: 600;">Primary Driver</th>
                    </tr>
                </thead>
                <tbody>
                    <tr style="background-color: var(--section-bg);">
                        <td style="padding: 15px; border-bottom: 1px solid var(--border-color);"><strong>0→1: Foundational Research</strong></td>
                        <td style="padding: 15px; border-bottom: 1px solid var(--border-color);">New ML Use Cases & System Needs</td>
                        <td style="padding: 15px; border-bottom: 1px solid var(--border-color);">Academia</td>
                    </tr>
                    <tr style="background-color: var(--background-color);">
                        <td style="padding: 15px; border-bottom: 1px solid var(--border-color);"><strong>1→2: Practical Enhancement</strong></td>
                        <td style="padding: 15px; border-bottom: 1px solid var(--border-color);">Scalability, Performance & Robustness</td>
                        <td style="padding: 15px; border-bottom: 1px solid var(--border-color);">Academia–Industry Collaboration</td>
                    </tr>
                    <tr style="background-color: var(--section-bg);">
                        <td style="padding: 15px;"><strong>2→∞: Efficiency Optimization</strong></td>
                        <td style="padding: 15px;">Aggressive Efficiency Squeeze & Cost Reduction</td>
                        <td style="padding: 15px;">Industry</td>
                    </tr>
                </tbody>
            </table>
            <p style="text-align: center; color: #666; font-size: 0.9em; margin-top: 1em; margin-bottom: 2em;"><strong>Table 1:</strong> Conceptual Stages of ML Systems Innovation</p>

            <ol>
                <li><strong>0→1:</strong> This represents the <em>most pioneering research</em>, often best pursued in <em>academic settings</em>. It involves addressing entirely new ML use cases, applications, or workloads for which no existing system adequately caters to their unique objectives, resource constraints, or workload characteristics. The novelty lies in being the first to identify these distinct system design requirements and to build a foundational solution. For instance, my work on Venn [3], Curie [4] and Exp-Bench [5] exemplify 0→1 research by identifying and addressing previously unarticulated system needs for emerging ML paradigms.</li>

                <li><strong>1→2:</strong> Once a foundational (0→1) system or concept exists, the next critical step is to <em>enhance its practicality and efficiency</em>. This phase focuses on making the system scalable, robust, and more performant. It's about transforming an initial proof-of-concept into something that is not just usable but also efficient under realistic conditions. This stage of research is also valuable and well-suited for <em>doctoral exploration and industry collaboration</em>, as it involves deep thinking and understanding of emerging ML workloads along with underlying resources to bridge the gap between novel concepts and practical applications. My contributions such as Andes [2], Fluid [1], and FedScale [7] fall into this category, building upon foundational ideas to deliver more performant systems capable of handling realistic workloads, and running with advanced resource scheduler and scaling to heterogeneous resources.</li>

                <li><strong>2→∞:</strong> This level involves the intensive effort required to make an ML system truly viable for large-scale, real-world industrial deployment. The primary focus here is on <em>aggressively optimizing efficiency</em>—cutting operational costs, maximizing resource utilization, and minimizing latency or training time. While critically important for widespread adoption, this phase often <em>demands substantial engineering resources and domain-specific expertise</em>, making it particularly well-suited for industry teams who possess the scale and focused incentives for such deep optimization. This distinction is perhaps particularly salient in ML systems, where academia and industry often collaborate or drive parallel innovations, especially with the rapid, real-world impact seen in fields like Generative AI. My projects like Auxo [6] and FedTrans [8] further improve the system and model efficiency via techniques like customized ML model training based on the underlying compute resources and data characteristics.</li>
            </ol>

            <h4 id="think-ahead">Think Ahead</h4>
            <p>Given the rapid pace of innovation in AI, it is crucial to think ahead and proactively position your research to address future needs. While ML systems play an indispensable role in enabling ML advancements—and can sometimes even drive new ML capabilities, akin to the 'hardware lottery' concept—they often serve a supporting role, following the advancements of ML models and algorithms. In fast-moving and competitive areas like Generative AI, where new ideas can quickly reshape the landscape, a reactive approach can leave research feeling disempowered. Therefore, it's vital for systems researchers not only to address current popular ML challenges but also to explore ML technologies and use cases that are likely to emerge and become significant <em>in the next three to five years</em>, or even further out. This foresight, as demonstrated by my pivot towards supporting Generative AI during my PhD, can lead to more impactful and enduring research contributions. Proactively seeking and integrating insights from industry trends, where possible, can further sharpen this forward-looking perspective.</p>

            <figure>
                <img src="figs/timeline.png" alt="Timeline of key focus areas in AI research" style="width: 70%; max-width: 800px; margin: 2em auto; display: block; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <figcaption style="text-align: center; color: #666; font-size: 0.9em; margin-top: 1em;">Figure 2: Timeline of key focus areas in AI research</figcaption>
            </figure>

            <h4 id="specialized-design">Specialized System Design</h4>
            <p>As AI continues to diversify, we encounter an expanding array of ML use cases, each possessing unique objectives and resource characteristics. These specialized demands present both unique challenges and significant opportunities for innovation. Therefore, when designing ML systems, it is often necessary to think from first principles—to challenge existing assumptions and identify the fundamental building blocks required for a given workload. For instance, my work on Fluid [1] highlighted the unique requirements of experimental model training, where the primary objective shifts from minimizing job completion time to optimizing makespan, as comprehensive evaluation across numerous training jobs is needed to identify optimal configurations. Similarly, Andes [2] was designed specifically for the emerging demands of AI conversational services, focusing on user-perceived Quality-of-Experience (QoE) rather than traditional system metrics such as request throughput.</p>
        </div>
    </div>

    <div class="section" id="future-work">
        <h2>Future Work</h2>

        <p>Looking ahead, the principles of user-centricity for pervasive AI and the lessons learned for ML systems research guide my vision for future work. The following directions focus on long-term opportunities that align with the anticipated trends in pervasive ML, primarily emphasizing foundational (0→1) and practicalization (1→2) research to pioneer and solidify the next generation of user-centric ML systems.</p>

        <div class="subsection" id="advancing-qoe">
            <h3>Advancing User-Centricity and Quality of Experience (QoE) Across Diverse Modalities</h3>

            <p>Inspired by the vision of pervasive AI (both off-body and on-body AI) and the proliferation of multimodal generative models, a primary direction for future work is to extend the principles of user-centric system design with the concept of Quality of Experience (QoE) as explored in Andes [2], to a broader spectrum of ML applications and modalities. As AI-driven generation and understanding of images, audio, and video become increasingly integrated into daily life, it is crucial to pivot system design objectives to <em>prioritize the user's direct experience with these rich media</em>.</p>

            <h4 id="qoe-metrics">Defining and Optimizing Modality-Specific QoE Metrics</h4>
            <p>Quantifying user experience for non-textual modalities requires thoughtful design. Future work must first formulate QoE metrics that align closely with the specific goals of underlying ML applications and the nuanced expectations of users.</p>
            <ol>
                <li><em>On the content quality side</em>, this includes the perceptual quality of generated images, the temporal coherence and narrative consistency of generated video, the fidelity of audio synthesis, and the accuracy and relevance of video understanding outputs—all from a user's perspective.</li>
                <li><em>On the system efficiency side</em>, this translates to optimizing for interactive responsiveness (e.g., time-to-first-token/image/frame, content delivery timeline), consistent content delivery across different modalities or transitions between them, overall system responsiveness under varying loads, and resource efficiency (e.g., energy, compute) in meeting target QoE levels.
                When it comes to on-body AI who acts subconsciously, a user-centric ML system should know the best timing of interaction based on various user's preferences. 
                </li>
            </ol>

            <p>Based on the defined QoE metrics, systems should ideally adapt QoE goals based on individual user preferences (e.g., tolerance for artifacts versus speed), task context (e.g., rapid prototyping versus final production), or even user expertise level. This necessitates research into adaptive scheduling algorithms that can dynamically adjust system behavior and resource allocation to meet these personalized QoE targets.</p>

            <h4 id="resource-management">Resource Management for Dynamic Generative Workloads</h4>
            <p>The computational demands of generative models, especially during interactive use, can be <em>highly variable and unpredictable</em>. For instance, an on-body AI assisting with visual tasks typically <em>consumes minimal resources during passive observation</em>. However, its resource demand can <em>spike dramatically when the user poses a complex query</em> (e.g., 'Summarize the key activities in this busy street market') about a dynamic and intricate environment. The complexity of both the environment (e.g., number of objects, rate of change) and the user's request (e.g., level of detail, reasoning required) directly dictates the necessary computational power for perception, understanding, and response generation.</p>

            <p>Additionally, the memory footprint, computational intensity, and access patterns across modalities vary significantly (e.g., LLMs versus image diffusion models versus vision encoder models), developing specialized system components and resource allocation strategies tailored to each modality is essential. This includes dynamic memory allocation for large models, adaptive batching strategies for variable arrival rates, specialized deployment strategies for different modalities, and efficient offloading mechanisms between edge and cloud resources. New resource management techniques and system designs will be needed to deliver responsive user interaction for such dynamic and resource-intensive generative tasks.</p>

            <h4 id="cross-modal">Cross-Modal QoE Synchronization</h4>
            <p>As applications increasingly blend multiple modalities (e.g., a robot assistant that visually perceives its environment, verbally plans its actions, and then physically interacts), ensuring <em>a consistent and high-quality experience across these interconnected components</em> will be a significant system design challenge. For instance, a system might need to ensure that visual understanding (e.g., identifying an object in a user's view) is tightly synchronized with concurrent auditory cues or interactive elements (e.g., highlighting the object on an AR display) to provide a seamless and coherent experience. This requires novel scheduling algorithms that understand inter-modal dependencies and optimize for a holistic and synchronized QoE.</p>
        </div>

        <div class="subsection" id="ai-agents">
            <h3>AI Agents for Self-Evolving ML Systems Design</h3>

            <p>The emergence of AI agents capable of performing complex tasks autonomously presents a transformative opportunity, potentially facilitating scientific research and accelerating innovation. My last-year work on building a co-scientist AI agent to help automate research experimentation and optimize the research solutions - Curie [4] - has shown the potential of this direction. Key capabilities and research opportunities include:</p>

            <ol>
                <li><strong>Automated System Design and Optimization:</strong> AI agents could be tasked with discovering optimal system configurations (i.e., 2→∞ research level tasks), such as the optimal degrees of different parallelisms (e.g., tensor, pipeline, data parallelism) for training LLMs, navigating vast and complex search spaces more effectively to find the optimal system configurations that maximize training throughput. Many 2→∞ tasks—those requiring significant, detailed engineering effort to squeeze out final percentages of performance or cost savings—could potentially be delegated to AI agents. This would free human researchers and engineers to focus on more foundational 0→1 or 1→2 challenges, accelerating the pace of innovation in ML systems. Beyond 2→∞ configuring systems, agents might propose and even implement novel system components or designs (contributing to 0→1 and 1→2 research levels). Such agents could propose innovative architectural components, autonomously adapt systems to evolving workloads and hardware, contributing to novel scheduling policies, kernel-level optimizations, or advanced strategies for computation-communication overlap.</li>

                <li><strong>Autonomous Benchmarking, Analysis, and Refinement:</strong> An AI agent could be empowered to deploy system changes, execute over diverse benchmark workloads, collect and analyze performance traces, and iteratively refine its designs based on observed outcomes, creating a closed loop of continuous system improvement. A fundamental challenge will be developing ways for AI agents to represent and understand the intricate components, traces, and performance characteristics of complex ML systems.</li>
            </ol>

            <p>Achieving this level of autonomous capability requires a new generation of foundation models, which are imbued with deep, specialized knowledge of ML systems. In addition, reinforcement learning is needed to train agents to master the full lifecycle of ML systems research experimentation. This necessitates high-quality datasets that capture end-to-end experimentation processes—including hypothesis generation, system implementation, execution, and analysis—to provide effective training supervision for such agents.</p>
        </div>

        <div class="subsection" id="next-gen-agents">
            <h3>Next-Generation Agentic AI Systems</h3>

            <p>As AI agents become capable of tackling <em>increasingly complex and long-running tasks</em>, the underlying system frameworks must evolve significantly. Current AI agents often rely on relatively simple sequences of API calls, but future agents will need to perform more sophisticated tool use (e.g., dynamically composing software libraries, executing generated code), interact robustly with physical environments via sensors, manage long-horizon tasks involving intricate dependencies and error recovery, and strategically leverage heterogeneous compute resources. This necessitates re-designing agentic AI system frameworks from the ground up to natively support these advanced agentic capabilities. A key focus will be on creating <em>abstractions that simplify the programming and orchestration of complex agentic workflows</em>. This includes:</p>

            <ol>
                <li><strong>Intelligent Resource Management:</strong> Agents will decompose high-level goals into many sub-tasks, each potentially requiring different computational resources (e.g., LLM inference, symbolic reasoning, code execution, physical actuators). The framework must provide abstractions to seamlessly <em>dispatch these sub-tasks to appropriate resources, whether local, cloud-based, or on specialized hardware</em>. To execute complex plans efficiently, agents will need to perform multiple sub-tasks, which may run in parallel or sequentially, often forming a Directed Acyclic Graph (DAG) of operations. The underlying system must offer intuitive ways to express and manage this parallelism, handling dependencies, data flow, and synchronization automatically where possible. Moreover, advanced scheduling are needed for interdependent sub-tasks and underlying resources to optimize for latency, cost, or other objectives, ensuring robust execution of long-running, multi-step agentic workflows. Finally, <em>robust checkpointing and fault tolerance mechanisms</em> will be essential to ensure the reliable execution of these potentially long-running and complex multi-step agentic workflows.</li>

                <li><strong>Self-Evolving Agent Systems:</strong> As agentic systems undertake longer-running tasks and operate in dynamic environments, they should possess the ability to learn and adapt continuously. The framework should support agents that learn from their past actions, environmental feedback, and direct human input. This involves integrating mechanisms akin to reinforcement learning, where agents can refine their policies, improve their planning abilities, and even discover new tools or strategies over time. Training AI agents that learn complex behaviors and adapt over long periods requires significant advancements in RL techniques and systems. This includes developing algorithms that are more sample-efficient to learn from limited interaction data, computationally scalable to handle complex state and action spaces, and capable of effective reward assignment over extended time horizons. From the systems perspective, this implies building resource-efficient infrastructure for distributed RL training and <em>memory management for past agent experiences</em>.</li>
            </ol>

            <p>By tackling these system-level challenges, we can enable the development of more capable, adaptable, and reliable AI agents that can address complex, real-world problems across a multitude of domains.</p>
        </div>
    </div>


    <div class="citation">
        <div class="citation-header">
            <h3>
                <svg class="header-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M12 2L13.09 8.26L20 9L13.09 9.74L12 16L10.91 9.74L4 9L10.91 8.26L12 2Z" fill="currentColor"/>
                </svg>
                Citation
            </h3>
            <button class="copy-btn" onclick="copyCitation()" title="Copy citation">
                <svg class="btn-icon" width="14" height="14" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M16 1H4C3 1 2 2 2 3V17H4V3H16V1ZM19 5H8C7 5 6 6 6 7V21C6 22 7 23 8 23H19C20 23 21 22 21 21V7C21 6 20 5 19 5ZM19 21H8V7H19V21Z" fill="currentColor"/>
                </svg>
                Copy
            </button>
        </div>
        <pre id="citation-text">@phdthesis{amberljc:dissertation,
    author          = {Jiachen Liu},
    title           = {User-Centric Machine Learning Systems},
    year            = {2025},
    month           = {June},
    institution     = {University of Michigan},
}</pre>
    </div>

    <div class="citation">
        <div class="citation-header">
            <h3>
                <svg class="header-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M14 2H6C4.9 2 4 2.9 4 4V20C4 21.1 4.89 22 5.99 22H18C19.1 22 20 21.1 20 20V8L14 2ZM18 20H6V4H13V9H18V20Z" fill="currentColor"/>
                </svg>
                References
            </h3>
            <button class="copy-btn" onclick="copyReferences()" title="Copy all references">
                <svg class="btn-icon" width="14" height="14" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M16 1H4C3 1 2 2 2 3V17H4V3H16V1ZM19 5H8C7 5 6 6 6 7V21C6 22 7 23 8 23H19C20 23 21 22 21 21V7C21 6 20 5 19 5ZM19 21H8V7H19V21Z" fill="currentColor"/>
                </svg>
                Copy All
            </button>
        </div>
        <p>[1] <a href="https://arxiv.org/abs/2105.11367">Fluid: Resource-Aware Hyperparameter Tuning Engine</a>, Peifeng Yu*, Jiachen Liu*, Mosharaf Chowdhury (* Equal contribution). MLSys 2021.</p>
        <p>[2] <a href="https://arxiv.org/abs/2404.16283">Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services</a>, Jiachen Liu, Zhiyu Wu, Jae-Won Chung, Fan Lai, Myungjin Lee, Mosharaf Chowdhury. Arxiv 2024.</p>
        <p>[3] <a href="https://arxiv.org/abs/2312.08298">Venn: Resource Management for Collaborative Learning Jobs</a>, Jiachen Liu, Fan Lai, Ding Ding, Yiwen Zhang, Mosharaf Chowdhury. MLSys 2025.</p>
        <p>[4] <a href="https://arxiv.org/abs/2502.16069">Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents</a>, Patrick Tser Jern Kon*, Jiachen Liu*, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Ang Chen (* Equal contribution). Arxiv 2025.</p>
        <p>[5] <a href="https://arxiv.org/abs/2505.24785">EXP-Bench: Can AI Conduct AI Research Experiments?</a>, Patrick Tser Jern Kon*, Jiachen Liu*, Xinyi Zhu, Qiuyi Ding, Jingjia Peng, Jiarong Xing, Yibo Huang, Yiming Qiu, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Matei Zaharia, Ang Chen (* Equal contribution). Arxiv 2025.</p>
        <p>[6] <a href="https://arxiv.org/abs/2210.16656">Auxo: Efficient Federated Learning via Scalable Cohort Identification</a>, Jiachen Liu, Fan Lai, Yinwei Dai, Aditya Akella, Harsha Madhyastha, Mosharaf Chowdhury. SoCC 2023.</p>
        <p>[7] <a href="https://arxiv.org/abs/2105.11367">FedScale: Benchmarking Model and System Performance of Federated Learning at Scale</a>, Fan Lai, Yinwei Dai, Sanjay S. Singapuram, Jiachen Liu, Xiangfeng Zhu, Harsha V. Madhyastha, Mosharaf Chowdhury. ICML 2022.</p>
        <p>[8] <a href="https://arxiv.org/abs/2404.13515">FedTrans: Efficient Federated Learning via Multi-Model Transformation</a>, Yuxuan Zhu, Jiachen Liu, Mosharaf Chowdhury, Fan Lai. MLSys 2024.</p>
        <p>[9] <a href="https://arxiv.org/abs/2504.16778">Evaluation Framework for AI Systems in "the Wild"</a>, Sarah Jabbour, Trenton Chang, Anindya Das Antar, Joseph Peper, Insu Jang, Jiachen Liu, Jae-Won Chung, Shiqi He, Michael Wellman, Bryan Goodman, Elizabeth Bondi-Kelly, Kevin Samy, Rada Mihalcea, Mosharaf Chowdhury, David Jurgens, Lu Wang. Arxiv 2025.</p>
        <p>[10] <a href="https://arxiv.org/abs/2312.03863">Efficient Large Language Models: A Survey</a>, Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang. TMLR 2024.</p>
    </div> 
    <!-- Visitor Counter at Bottom -->
    <!-- <div id="visitor-count" style="text-align: center; margin: 20px 0 10px auto; padding: 8px; background-color: var(--section-bg); border-radius: 6px; font-size: 0.75em; color: var(--light-text); max-width: 200px; box-shadow: 0 1px 2px rgba(0,0,0,0.1);">
        👁️ You're the <span id="page-views">...</span>th reader.
    </div> -->

</body>
</html>
